[
  {
    "objectID": "risk.html",
    "href": "risk.html",
    "title": "What is risk?",
    "section": "",
    "text": "Imagine that you go to your doctor. You’d like to know your (individual!) risk for some cancer. Let’s talk about what this actually means. Your risk is defined as the probability that you will actually get that disease. The concept of “true risk” is debated because you either will develop cancer or you won’t1 (see the “problem of the single-case” below). For now, we assume that this is not pre-determined. This estimate for your true risk is practical because 1) neither you, the best researchers, nor the smartest clinicians can be sure that you definitely will or will not develop cancer, and 2) it may prompt you to take preventive behaviors, such as increased screenings or changing your modifiable risk factors (e.g. diet and exercise), that could help reduce your risk.\nAssuming that a true risk does exist, let’s talk about your hypothetical probability of developing cancer. We’ll let this little blue circle represent a healthy version of you:\n\n\n\n\n\n\nMy goal as a researcher is to estimate your (individual) probability of getting cancer. In a statistically-perfect, fictitious universe, I would study many (let’s say 100) counterfactual versions of you. These counterfactuals would be identical to you in every way. Let me repeat: in each world, all known and unknown characteristics that make you you, would be replicated perfectly.\n\n\n\n\n\n\nThen, in each world, I would monitor you to see if you develop cancer. For this demonstration, let’s say you develop cancer in 12 out of 100 worlds. We would thus estimate that your probability of developing cancer is \\(\\frac{12}{100} * 100\\% = 12\\%\\).\n\n\n\n\n\n\nLet’s stop right there. First of all - why do we get different results in different worlds? As an empiricist, I find this apparent lack of reproducibility deeply unintuitive, even troubling. Secondly, did you notice that I estimated a probability as an observed proportion - why did I do that? Thirdly, once we have an estimate for your probability of disease, what does it even mean? Finally, in the real world, we can’t study 100 counterfactual versions of you, so what do we do? This discussion aims to address each of these points (at somewhat inconsistent levels of depth). Peruse various sections depending on your interests."
  },
  {
    "objectID": "risk.html#the-set-up",
    "href": "risk.html#the-set-up",
    "title": "What is risk?",
    "section": "",
    "text": "Imagine that you go to your doctor. You’d like to know your (individual!) risk for some cancer. Let’s talk about what this actually means. Your risk is defined as the probability that you will actually get that disease. The concept of “true risk” is debated because you either will develop cancer or you won’t1 (see the “problem of the single-case” below). For now, we assume that this is not pre-determined. This estimate for your true risk is practical because 1) neither you, the best researchers, nor the smartest clinicians can be sure that you definitely will or will not develop cancer, and 2) it may prompt you to take preventive behaviors, such as increased screenings or changing your modifiable risk factors (e.g. diet and exercise), that could help reduce your risk.\nAssuming that a true risk does exist, let’s talk about your hypothetical probability of developing cancer. We’ll let this little blue circle represent a healthy version of you:\n\n\n\n\n\n\nMy goal as a researcher is to estimate your (individual) probability of getting cancer. In a statistically-perfect, fictitious universe, I would study many (let’s say 100) counterfactual versions of you. These counterfactuals would be identical to you in every way. Let me repeat: in each world, all known and unknown characteristics that make you you, would be replicated perfectly.\n\n\n\n\n\n\nThen, in each world, I would monitor you to see if you develop cancer. For this demonstration, let’s say you develop cancer in 12 out of 100 worlds. We would thus estimate that your probability of developing cancer is \\(\\frac{12}{100} * 100\\% = 12\\%\\).\n\n\n\n\n\n\nLet’s stop right there. First of all - why do we get different results in different worlds? As an empiricist, I find this apparent lack of reproducibility deeply unintuitive, even troubling. Secondly, did you notice that I estimated a probability as an observed proportion - why did I do that? Thirdly, once we have an estimate for your probability of disease, what does it even mean? Finally, in the real world, we can’t study 100 counterfactual versions of you, so what do we do? This discussion aims to address each of these points (at somewhat inconsistent levels of depth). Peruse various sections depending on your interests."
  },
  {
    "objectID": "risk.html#what-is-randomness-and-where-does-it-come-from",
    "href": "risk.html#what-is-randomness-and-where-does-it-come-from",
    "title": "What is risk?",
    "section": "2. What is randomness and where does it come from?",
    "text": "2. What is randomness and where does it come from?\nTrue randomness is the absolute lack of pattern or predictability of events, and it is arguably an intrinsic aspect of natural systems2:\n\nQuantum mechanics states that we cannot precisely know both the position and velocity of subatomic particles, and by measuring either, we increase the uncertainty around the other quantity. Practically, this means that we cannot predict radioactive decay times for individual atoms, Brownian motion, heat transfer patterns, or cosmic radiation, to name a few. Thus, quantum mechanics serves as an inherent source of randomness in the universe.\n\nAlternatively, apparent randomness can be debunked by taking a closer look at the system, upon which order, pattern, and/or predictability could be discerned:\n\nChaos is unpredictability of a system arising due to high sensitivity to initial conditions. Given the slightest change in subatomic or atomic factors, we can see wide variations in outcomes (i.e. the butterfly effect).\nStochasticity is the idea that a very large number (i.e. an uncountable number) of external agents interact, creating an extremely complex web of factors that affect an outcome.\n\nApparently random processes are extremely complex. Sometimes we approximate them with true randomness to simplify our model and minimize required computational power. For example, genetic mutations and environmental exposures (e.g. air pollution, smoking, diet, exercise, UV exposure, etc.) may not be truly random3, but we can simplify our calculations dramatically by selecting only the most important risk factors and grouping the others into a random error term."
  },
  {
    "objectID": "risk.html#why-do-we-use-a-proportion-to-estimate-a-probability",
    "href": "risk.html#why-do-we-use-a-proportion-to-estimate-a-probability",
    "title": "What is risk?",
    "section": "3. Why do we use a proportion to estimate a probability?",
    "text": "3. Why do we use a proportion to estimate a probability?\nThe field of statistics gives us several tools to estimate a parameter, which is exactly what our probability is here. Let’s begin by re-stating our problem mathematically. We will consider your health status (yes, yours!) as a random variable since we don’t know your outcome yet. Your possible outcomes are healthy (denoted as a \\(0\\)) and diseased (denoted as a \\(1\\)). Thus, we aim to estimate your probability of being diseased, \\(p\\), defined as \\(p = P(X = 1)\\), where \\(P\\) is a probability function. Thus, since we have two possible outcomes, and one probability parameter, we will assume that \\(X\\) follows a Bernoulli distribution with probability p:\n\\[X \\sim Ber(p)\\]\n\nA. Method of moments estimation\nThe method of moments estimation technique is based on the result from the Weak Law of Large Numbers (WLLN), which says that the empirical mean \\(\\bar{X}\\) approaches the true mean of a random variable [X] as the number of observations \\(n\\) gets large. More formally, we state the WLLN as follows:\n\\[\n\\bar{X_n} \\to^p \\mathbb{E}[X]\n\\] where \\(\\to^p\\) indicates convergence in probability.\nThus, we can substitute the observed empirical mean \\(\\bar{x}\\) for its expectation, and solve for our parameter \\(p\\) as follows:\n\\[\n\\begin{align}\n\\mathop{\\mathbb{E}}[X] &= p \\\\\n\\bar{X} &\\approx p \\\\  \n\\hat{p} &= \\frac{1}{n}\\sum_{i=1}^{n}X_i\n\\end{align}\n\\]\n\n\nB. Maximum likelihood estimation\nAn alternative approach, that has many appealing properties, is known as maximum likelihood estimation, or MLE. This method reframes the problem in such as way that we can think about finding the parameter \\(p\\) that makes our observations \\(x\\) most likely. For a \\(Ber(p)\\) random variable, the likelihood function, which is the same as the joint probability mass function, is\n\\[\n\\begin{align}\n{\\cal{L}}{(p|X)} &= \\Pi_{i=1}^n P(X_i|p)  \\\\\n&= \\Pi_{i=1}^{n} p^{X_i}(1-p)^{1 - X_i}\n\\end{align}\n\\]\nWe will maximize the log likelihood (since it is easier to work with), by taking its derivative, setting it equal to zero, and solving for \\(p\\):\n\\[\n\\begin{align}\nlog({\\cal{L}}) &\\propto \\sum_{i=1}^{n} X_i log(p) + (1-X_i)log(1-p) \\\\\n\\frac{dlog{\\cal{L}}}{dp} &\\propto \\frac{\\sum_{i=1}^{n}{X_i}}{p} - \\frac{n-\\sum_{i=1}^{n}{X_i}}{1-p}  \\\\\n0 &= \\frac{\\sum_{i=1}^{n}{X_i}}{p} - \\frac{n-\\sum_{i=1}^{n}{X_i}}{1-p}  \\\\\n\\hat{p} &= \\frac{1}{n}\\sum_{i=1}^{n}{X_i}\n\\end{align}\n\\]\nIn summary, both methods give the same estimate for \\(p\\) which is equal to the total number of successes (or in this case, the number of times you develop the disease: 12) divided by the total number of trials (or in this case, the total number of fictitious worlds where we followed a counterfactual version of you: n = 100). Thus, we arrive at our previous estimate that you have a 12% probability of getting cancer."
  },
  {
    "objectID": "risk.html#what-is-a-probability-anyway",
    "href": "risk.html#what-is-a-probability-anyway",
    "title": "What is risk?",
    "section": "4. What is a probability anyway?",
    "text": "4. What is a probability anyway?\nNow that I’ve told you that your true risk of developing cancer is 12% - so what?? What are some ways you can interpret that statement?\nIntuitively, many of us think about probability as the chance or likelihood of something happening. More rigorously, recall that a probability measure must meet the three axioms stated by Kolmogorov (1933). Simply put, they say that 1) the total probability of all possible outcomes must equal 1, 2) for any possible outcome, its probability must be between 0 and 1 (i.e. no negative probabilities), and 3) that the probability of mutually exclusive outcomes is equal to the sum of the probabilities of each outcome. Mathematically, we write:\nFor probability triple, \\((\\Omega, \\cal{F}, P)\\), where \\(\\Omega\\) = sample space, \\(\\cal{F}\\) = \\(\\sigma\\)-algebra, and \\(P\\) = probability measure, \\[P(\\Omega) = 1, \\] \\[P(p) \\ge 0 \\text{ for any } p \\in \\Omega,\\] \\[P(\\cup_{i} p_i) = \\Sigma_{i} p_i \\text{ for countable disjoint } p_i.\\]\nThere are at least six distinct ways to interpret probabilities4, but I will highlight the three most common. I won’t go into each thoroughly, but I want to give a sense for the range of what a probability might mean. Perhaps the similarities and differences between the interpretations will help us understand the concept of probability more deeply. I think it’s important for the person communicating the probability as well as the one receiving it to be aware of the potential ways that the information may be interpreted.\n\nA. Classical probability\nCore idea: Probability is determined a priori by identifying all possible outcomes and assigning each one an equal chance.\nExample: Assume a coin is fair. Then since there are two sides (heads and tails), we attribute equal probability to each: \\(P(heads) = P(tails) = 0.5\\).\nBreakdown: Large (uncountable) probability spaces, spaces with unknown/unaccounted outcomes, unequal chances for different events (e.g. weighted dice, unfair coin)\n\n\n\nHeads and tails of a U.S. quarter\n\n\n\n\nB. Frequency interpretations\nCore idea: Frequentists assume that the long-run frequency of events is fixed, objective, and intrinsically tied to the probability of that event. Thus, instead of considering all of the possible outcomes (as in classical probability), we will use the observed outcomes and compare this to the expected results assuming a null hypothesis4.\nExample: Let’s assess whether a coin is fair by estimating the probability of a coin landing on heads. We would flip the coin many times and estimate the probability by counting the number of heads and dividing it by the number of total trials. If this is (sufficiently) close to 0.5, we would assume this is a fair coin. Experiments with a large number of repeated trials or large sample sizes of independent events are good applications for frequentists statistics.\nBreakdown: Suppose we only observed a few of the trials above. If we observed up to 10 trials (first row below) we would be tempted to conclude that your cancer risk is 0/10 = 0%. However, if we observed 11 trials, we would conclude that your risk is 1/11 = 9.1%. With 50 trials, we would estimate that your cancer risk is 4/50 = 8%. But none of these is correct. We know theoretically that a sequence of unlikely events is possible, but frequentists assume that the probability that corresponds most closely with the observed data, regardless of prior knowledge, is the best estimate. As the number of trials increases, this becomes less of a problem, but few real-life events are repeatable an arbitrary number of times, and many are downright unrepeatable (e.g. the Big Bang, volcanic eruption/earthquake/natural disaster, 2024 presidential election, etc.) leaving us with a gap in how to reconcile the true probability of the event. This is known as the “problem of the single-case.” This also becomes much more complex in the event of dependent events.\n\n\n\n\n\n\n\n\nC. Subjective interpretation (aka Bayesian)\nCore idea: A probability represents someone’s degree of belief, confidence, or credence. This “someone” is a rational and logically consistent “suitable agent” who follows the axioms of probability. However, these credences are inherently subjective (hence the name). Your level of credence about an event can be estimated by what you would consider a “fair bet” (i.e. putting your money where you mouth is): “Your degree of belief in E is p iff p units of utility is the price at which you would buy or sell a bet that pays 1 unit of utility if E, 0 if not E”4. In other words, you are indifferent to placing a bet \\(p\\) on \\(E\\) as you are to placing \\(1-p\\) on \\(\\neg E\\). In the statistical framework, someone’s belief is based on prior knowledge - summarized as a prior probability with a selected distribution and parameter(s) - which is combined with observed data to obtain a posterior, from which conclusions are drawn5.\n\\(\\text{Bayes Rule:}\\) \\[\nP(H|E) = \\frac{P(E|H) \\cdot P(H)}{P(E)}\n\\] where \\(H\\) is the hypothesis, \\(P(H)\\) is the estimate of the probability of the hypothesis (i.e. “prior” probability), \\(E\\) is the evidence (i.e. data), \\(P(E)\\) is the marginal likelihood of the data, \\(P(E|H)\\) is the “likelihood” of the data given the fixed hypothesis, and \\(P(H|E)\\) is the probability of the hypothesis given the fixed data (i.e. “posterior” probability).\nExample: We could evaluate the hypothesis that a coin is fair compared to the hypothesis that it is not by calculating the posterior under each prior. First we might assume that the coin flip is fair (i.e. the flip is distributed Bernoulli(0.5)). Then we flip the coin many times and count the number of heads. We then estimate the likelihood of this event given the fixed hypothesis that the coin is fair. Finally, we multiply the likelihood and the prior to generate a posterior probability that the coin is fair given the data. It is common to compare two hypotheses, so we may also repeat the process assuming another parameter for the prior probability. Then we could compare the posterior probabilities for these two hypotheses and see which is more probable.\nBreakdown: Selecting a prior is subjective and there are few well-defined methods for choosing one6. Sometimes there is minimal prior knowledge, in which case a uniform prior may be used; this is the same as doing a frequentist analysis.\nGoing back to our cancer example, we employed a frequentist approach to estimate your risk of cancer (in some fictitious universe). If we had a priori knowledge about your risk of cancer, e.g. given your family history, age, or other factors, we could update our probability estimate by combining the observed data and this prior knowledge in a Bayesian approach."
  },
  {
    "objectID": "risk.html#how-do-we-actually-estimate-your-probability-of-disease",
    "href": "risk.html#how-do-we-actually-estimate-your-probability-of-disease",
    "title": "What is risk?",
    "section": "5. How do we actually estimate your probability of disease?",
    "text": "5. How do we actually estimate your probability of disease?\nIn reality, of course, we can’t study 100 counterfactual versions of you. We have to make an estimate with what’s available to us now. Since you are a human, let’s estimate the risk within a group of 100 randomly sampled humans:\n\n\n\n\n\n\nAfter following these 100 individuals, we find that 11 people get cancer. Risk (or cumulative incidence) is estimated as the number of incident (i.e. new) cases within a given time divided by the number of individuals in the study population (sounds familiar, eh?). Thus, we estimate an 11% risk within this study population.\n\n\n\n\n\n\nThen, assuming that all individuals in this study population are independent and identically distributed*, using the math above, we assign each individual within the target population (which includes you!) the same probability of disease. * The assumption of the participants being identically distributed is critical, and impossible to test. Practically, researchers and clinicians attempt to group individuals with similar risk factors into the same group to estimate risk, but defining which population is most relevant to you is challenging if not impossible."
  },
  {
    "objectID": "risk.html#the-reference-class-problem",
    "href": "risk.html#the-reference-class-problem",
    "title": "What is risk?",
    "section": "6. The reference class problem",
    "text": "6. The reference class problem\nFor demonstration purposes, let’s assume the same overall risk of 11% in the total population. Now consider the subset of this population that are men (i.e. the left half of the population): 8/50 men get sick, so their risk is 16%. Women (on the right), however, get sick at a rate of 3/50 = 6%. We can subdivide our population further by age, or any other risk factors (or any combination of these characteristics), and we estimate different risks for each subpopulation. Taking this to an extreme, we see that a young man has a 20% risk, and an older man has a 12% risk; a young woman has an 8% risk, and an older woman has a 4% risk. So which population - and consequently which risk estimate - is most relevant for a given individual?\n\n\n\n\n\n\nThis predicament of identifying the appropriate population for which to estimate risk is known as the “reference class problem.” This problem was identified by John Venn in 1876: “every single thing or event has an indefinite number of properties or attributes observable in it, and might therefore be considered as belonging to an indefinite number of different classes of things”7. Thus, Stern (2012)1 argues that there is no such thing as unconditional or individual risk. The risk factors included in the model that we select determine the population classes, and thus the risk estimates for each class. Even for well-calibrated models, when we compare two models, the risk estimates associated with any one individual can vary substantially depending on which reference class an individual is assigned to8."
  },
  {
    "objectID": "risk.html#limitations",
    "href": "risk.html#limitations",
    "title": "What is risk?",
    "section": "7. Limitations",
    "text": "7. Limitations\nThe goal of precision medicine is to make the reference class as small as possible so that there is less variability within that group, and better estimates of risk are assigned to each individual9. This is a tricky problem and is discussed thoroughly by Kent et al. (2018). Most importantly, risk should be clearly communicated to doctors and patients that risk is estimated using a specific model, and “people with X, Y, and Z risk factors have a W% risk for disease Q.”\nIn my research, however, we skirt the individual risk issue by making predictions at the population-level. We can feel confident making these predictions when the model is shown to perform well on a given population (i.e. it is “well-calibrated”). Thus, our models can help inform how cancer screening guidelines should set to screen the top X percentage of the population at highest risk, or other allocations of resources.\nAdditional limitations include the problem of adjusting someone’s risk if their covariates change over time - it is currently unknown how this will affect risk since most models only include covariates measured at baseline. Issues surrounding generalizability or transportability to other populations for groups that are under-represented in study populations are also common."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Example analysis",
    "section": "",
    "text": "The dark-eyed junco was the most commonly observed species in Project FeederWatch in winter 2020-21.  Photo credit: Bob Vuxinic, submitted to Project FeederWatch\n\n\n\nProject aims\nIn this example analysis, I investigate patterns in abundance and distribution of birds in the 2020-21 winter in the United States and southern Canada. Specifically, I aim to answer the following questions:\n\nwhen and where are birds observed\nwhich species are most common, and where are they found\nwhich species tend to occur in large flocks, and where are they found\n\nThis analysis is intended for the general public, as well as the citizen scientists (and the graders for Biostat 777).\n\n\nData from Project FeederWatch\nThese data were discovered via TidyTuesday for January 10, 2023, but originally come from Project FeederWatch. Project FeederWatch is a citizen science project that aims to engage individuals in North America to count birds (for as long or as little as they like) to track winter trends in bird distribution and abundance. This project has been running for more than 30 years at this point!1 The data dictionary is available here.\n\n\nShow the code\n# Load data and do quick exploration/clean-up\n\n# List required files to download from TidyTuesday\nrds_files &lt;- c(\"feederwatch.RDS\",\"site_data.RDS\")\n\n# Check if any of these files don't exist\nif (any(!file.exists(here(\"data\", rds_files)))) {\n  dir.create(here(\"data\"))\n# if missing, download the data\nfeederwatch &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-01-10/PFW_2021_public.csv')\nsite_data &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-01-10/PFW_count_site_data_public_2021.csv')\n\n# save the data objects as RDS files\nsaveRDS(feederwatch, file = here(\"data\",\"feederwatch.RDS\"))\nsaveRDS(site_data, file = here(\"data\",\"site_data.RDS\"))\n}\n\n# Load data\nfeederwatch &lt;- readRDS(here(\"data\",\"feederwatch.RDS\"))\nfeederwatch_og &lt;- feederwatch\nsite_data &lt;- readRDS(here(\"data\",\"site_data.RDS\"))\nsite_data_og &lt;- site_data\n# had to save the species_codes dictionary manually from https://docs.google.com/spreadsheets/d/1kHmx2XhA2MJtEyTNMpwqTQEnoa9M7Il2/edit#gid=2040245914\nspecies_codes_dd &lt;- read_csv(here(\"data\",\"FeederWatch_Data_Dictionary_Species_Codes.csv\"),skip = 1)\n\n\n# Quick data exploration, clean up, and creation of a few useful data frames \n# glimpse(feederwatch)\n# summary(feederwatch)\n#length(unique(feederwatch$species_code))\n\n# Change format for names in data dictionary:\nspecies_codes_dd$PRIMARY_COM_NAME &lt;- str_to_title(species_codes_dd$PRIMARY_COM_NAME)\n\n# Check for outliers in space\nlow_lat &lt;- which(feederwatch$latitude&lt;0)   # length=1; this point appears to be erroneous - remove it below\nhigh_lat &lt;- which(feederwatch$latitude&gt;50) # these are not erroneous, but want to narrow to US for plotting later on\n\n# Get rid of the one low-latitude observation\nfeederwatch &lt;- feederwatch[-low_lat,]\n# hist(feederwatch$latitude)\n\n# Add latitudinal bands to the data\nfeederwatch_df &lt;- feederwatch %&gt;%\n  mutate(Date = ymd(paste(Year, Month, Day, sep=\"-\")),\n         lat_band = case_when(\n           latitude &lt; 35 ~ \"low\",\n           latitude &gt;=35 & latitude &lt; 45 ~ \"mid\",\n           latitude &gt;= 45 ~ \"high\"\n         )) \nfeederwatch_df$lat_band &lt;- factor(feederwatch_df$lat_band, levels=c(\"high\",\"mid\",\"low\"))\n\n# Make useful dataframes:\n# Total bird count per day \nbird_count_per_day &lt;- feederwatch_df %&gt;%\n  group_by(Date, \n           lat_band) %&gt;%\n  summarize(total = sum(how_many),\n            count = n(),\n            std_total = total/count)\n\n# Abundant birds: birds that have highest counts in the dataset - i.e. flocking birds\nabundant_birds &lt;- feederwatch_df %&gt;%\n  filter(latitude&lt;50) %&gt;%\n  group_by(species_code) %&gt;%\n  summarize(total = sum(how_many),\n            count = n(),\n            std_total = total/count) %&gt;%\n  arrange(desc(std_total)) %&gt;%\n  head(n=9) %&gt;%\n  left_join(species_codes_dd, \n            by=join_by(species_code==SPECIES_CODE)) %&gt;%\n  select(species_code, \n         PRIMARY_COM_NAME, \n         SCI_NAME, count)\n\n# join with species info:\nabundant_birds_df &lt;- feederwatch_df %&gt;%\n   filter(species_code %in% abundant_birds$species_code) %&gt;%\n   left_join(species_codes_dd, \n             by=join_by(species_code==SPECIES_CODE)) %&gt;%\n   mutate(Month=factor(Month, levels=c(\"11\",\"12\",\"1\",\"2\",\"3\",\"4\")))\n\n\n# Common birds: birds that have several records in the dataset\ncommonly_sighted_birds &lt;- feederwatch_df %&gt;%\n  group_by(species_code) %&gt;%\n  summarize(total = sum(how_many),\n            count = n(),\n            std_total = total/count) %&gt;%\n  arrange(desc(count)) %&gt;%\n  head(n=9) %&gt;%\n  left_join(species_codes_dd, \n            by=join_by(species_code==SPECIES_CODE)) %&gt;%\n  select(species_code, \n         PRIMARY_COM_NAME, \n         SCI_NAME, \n         count)\n\n# join with species info:\ncommonly_sighted_birds_df &lt;- feederwatch_df %&gt;%\n   filter(species_code %in% commonly_sighted_birds$species_code) %&gt;%\n   left_join(species_codes_dd, \n             by=join_by(species_code==SPECIES_CODE)) %&gt;%\n   mutate(Month=factor(Month, levels=c(\"11\",\"12\",\"1\",\"2\",\"3\",\"4\")))\n\n\n\n\nWhere are birds observed?\nIn order to understand trends in bird distribution and abundance, we first need to know where our observations are coming from. Here is a figure showing the locations of observations. The points are transparent, so denser bird-watching areas are shaded darker green.\n\n\nShow the code\n# Figure 1: Map of unique sighting locations\n\n# Prep maps of America/USA for making figures\namerica_map &lt;- map_data(\"world\", region='USA')\nUSA_map &lt;-\n  ggplot(america_map, \n         aes(x=long, y=lat, group=group)) +\n  geom_polygon(fill=\"white\",color=\"black\") + \n  scale_x_continuous(limits = c(-125,-65)) +\n  scale_y_continuous(limits = c(25, 50)) + \n  coord_map() +\n  theme_minimal()\n\n# Identify unique sighting locations\nunique_sighting_locations &lt;- feederwatch %&gt;%\n  group_by(loc_id) %&gt;%\n  summarize(n_obs = n(),\n            latitude=mean(latitude),\n            longitude=mean(longitude))\n\n# Plot unique sighting locations\nUSA_map +\n  geom_point(data=unique_sighting_locations, \n             aes(x = longitude, y = latitude, group=NULL), \n             alpha=0.1, \n             color=\"darkgreen\") +\n  labs(x=\"Longitude (ºE)\", \n       y = \"Latitude (ºN)\", \n       title = \"Observing locations for 2020-21 winter in US and southern Canada\",\n       subtitle = \"Engagement in Project Feederwatch is stronger in the eastern than the western U.S., \\nand the highest density of birdwatchers are in the mid-Atlantic to Northeast regions.\",\n       caption = \"Data: Project FeederWatch\")\n\n\n\n\n\n\n\n\nFigure 1: Observing locations for Project FeederWatch 2020-21 winter.\n\n\n\n\n\n\n\n\n\n\n\nBirdwatching locations\n\n\n\nParticipants can birdwatch anywhere – it doesn’t have to be at their backyard feeder! But remember the distribution of observing locations in this figure since it will be relevant for understanding the distribution of common and flocking birds examined below.\n\n\nSince most bird-watching happens in the mid-latitudes (defined here as 35-45ºN), we must standardize the number of birds by the number of observations. The following figure indicates that birds are more abundant at higher latitudes even though there are fewer total observations. However, birds become more common at lower latitudes (&lt;35ºN) from January to March. This may be explained by migration to southern regions during the coldest months of the year. The fact that more birds are reported within every latitudinal band during the coldest months may indicate that more birds visit feeders to obtain food during the harshest conditions of the year.\n\n\n\n\n\nShow the code\n# Figure 2: Bird count per latitude band\nbird_count_per_day %&gt;%\n  ggplot(aes(x=Date, y = std_total, color = lat_band)) +   \n  geom_smooth() +   # loess-smoother is default\n  theme_minimal() +   \n  labs(y = \"Average number of birds per observation\", \n       title = \"Birds observed per latitude band during 2020-21 winter\", \n       color = \"Latitude band\", \n       subtitle = \"Birds are most abundant at high latitudes, but become relatively \\nmore frequent in low latitudes in the winter\", \n       caption = \"Data: Project FeederWatch\")\n\n\n\n\n\n\n\n\nFigure 2: Total number of birds observed within each latitudinal band (high: &gt;=45ºN, mid: 35-45ºN, low: &lt;35ºN).\n\n\n\n\n\n\n\nLOESS (locally weighted running line smoother) was used on these curves. This is the default option for geom_smooth.\n\n\n\nLet’s now explore how often different species are observed. A total of 361 unique species were reported, but most of these species were only observed once! However, the most common species were observed thousands of times! This is probably explained in part because birds are incredibly diverse,2 so some of these species may actually be rare, while others have may have limited distributions or be harder to identify.\n\n\nShow the code\n# Figure 3: Histogram of bird observations per species\nfeederwatch_df %&gt;%\n  group_by(species_code) %&gt;%\n  summarize(n_obs = n()) %&gt;%\n  ggplot() +\n  geom_histogram(aes(n_obs), \n                 boundary=0, \n                 closed=\"left\") +\n  labs(x = \"Number of observations for a given species\", \n       y = \"Frequency\", \n       title = \"Number of observations per bird species\",\n       subtitle = \"Most bird species are only observed once. A few common species are observed \\nthousands of times.\",\n       caption = \"Data: ProjectFeederwatch\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 3: Histogram of the number of observations per species.\n\n\n\n\n\n\n\n\n\n\n\nRare birds are becoming rarer\n\n\n\nIn 2019, the National Audubon Society estimated that two-thirds of North American birds were at risk of extinction due to climate change.3\n\n\n\n\nWhich birds are most common?\nYou’ll probably recognize the names of some of the most common birds! These birds were identified as “common” in the data set because they had the most observations (though not necessarily the highest total count of individuals).\n\n\nShow the code\n# Table 1\ncommonly_sighted_birds_table &lt;- commonly_sighted_birds %&gt;%\n  select('Common name' = PRIMARY_COM_NAME,\n         'Scientific name' = SCI_NAME,\n         'Number of observations' = count)\ncommonly_sighted_birds_table\n\n\n\nMost commonly observed birds in the 2020-21 winter in Project FeederWatch.\n\n\n\n\n\n\n\nCommon name\nScientific name\nNumber of observations\n\n\n\n\nDark-Eyed Junco\nJunco hyemalis\n5465\n\n\nDowny Woodpecker\nPicoides pubescens\n5449\n\n\nNorthern Cardinal\nCardinalis cardinalis\n5438\n\n\nHouse Finch\nHaemorhous mexicanus\n4955\n\n\nMourning Dove\nZenaida macroura\n4938\n\n\nBlack-Capped Chickadee\nPoecile atricapillus\n4696\n\n\nBlue Jay\nCyanocitta cristata\n4578\n\n\nAmerican Goldfinch\nSpinus tristis\n4446\n\n\nWhite-Breasted Nuthatch\nSitta carolinensis\n4393\n\n\n\n\n\nMost of the common birds were observed throughout the U.S. (or at least where people were reportedly birdwatching, see Figure 1). However, some birds had more limited distributions. For example, the blue jay and the northern cardinal were found predominantly in the eastern half of the U.S., and the black-capped chickadee was only observed in the more northern latitudes.\n\n\nShow the code\n# Figure 4: Commonly sighted birds\n# Since this figure takes a long time to render, we'll just load in the saved figure below, but this is the code that was used to generate it:\n  \ncommon_birds_plot &lt;- USA_map +\n  geom_bin_2d(data = commonly_sighted_birds_df, \n              aes(x = longitude, y = latitude, group=NULL)) +\n  scale_fill_continuous(low=\"lightskyblue1\", \n                        high=\"indianred4\", \n                        name=\"Number of \\nsightings\") +\n  theme_minimal() + \n  labs(x=\"Longitude (E)\", \n       y=\"Latitude (N)\", \n       title=\"Observations of common birds in the US and southern Canada\", \n       subtitle=\"Common bird species are most frequently observed in the Mid-Atlantic and \\nNortheast regions, likely due (in part) to a sampling bias and easy bird identification\", \n       color=\"Month\") + \n  facet_wrap(vars(PRIMARY_COM_NAME), \n             labeller = labeller(PRIMARY_COM_NAME = label_wrap_gen(width = 10)))\n#ggsave(here(\"figs\",\"Commonly_sighted_birds.png\"), plot=common_birds_plot, width=7, height=7, units=\"in\")\n\n\n\n\n\n\n\n\nFigure 4: Heatmap of observations for the most common birds.\n\n\n\n\n\nWhich birds are usually found in a flock?\nHave you heard the saying that “birds of a feather flock together”? The birds in the figure below had the highest average abundance per observation. This indicates to me that these birds tend to appear most often in large flocks.\n\n\nShow the code\n# Figure 5: Most abundant birds (i.e. probably largest flocks)\n# focus on birds in US (or below 50oN):\nmonth_colors = c(\"sandybrown\",\"indianred\",\"slateblue3\",\"royalblue\",\"palegreen3\")\n\nUSA_map +\n  geom_point(data = abundant_birds_df, \n             aes(x = longitude, y = latitude, group=NULL, size=how_many, color=Month), \n             alpha=0.5) +\n  scale_color_manual(values=month_colors) +   \n  theme_minimal() + \n  labs(x=\"Longitude (E)\", \n       y=\"Latitude (N)\", \n       title=\"Flocking birds observed in the US and southern Canada\", \n       subtitle=\"Birds with the highest average count per observation \\ntend to have limited distributions\", \n       caption = \"Data: Project FeederWatch\",\n       size=\"Count per observation\") +\n  facet_wrap(vars(PRIMARY_COM_NAME), \n             labeller = labeller(PRIMARY_COM_NAME = label_wrap_gen(width = 10)))\n\n\n\n\n\n\n\n\nFigure 5: Month and location for bird species with the largest average count per observation. The size of the circle is relative to the flock size, and the color indicates the month when the observation was made.\n\n\n\n\n\n\n\nSummary\nIn this example analysis, we explored patterns in the distribution and abundance of birds observed through the citizen science initiative Project FeederWatch conducted in winter 2020-21. We found that most of the observations for this program were made in the mid-Atlantic and northeast regions of the U.S. This pattern of observation corresponded strongly with the distribution of several of the most common birds, including the American goldfinch, dark-eyed junco, downy woodpecker, and house finch. However, flocking birds tended to have smaller ranges but a larger number of birds reported per observation. In general, birds were most abundant in the coldest months (January-March) and at the highest latitudes (&gt;45ºN).\n\n\nFunctions used\ndplyr: mutate, group_by, summarize, arrange, filter\nggplot2: geom_smooth, geom_histogram, geom_polygon, geom_bin_2d, geom_point, facet_wrap\n\n\n\n\n\nReferences\n\n1. Bonter, D. N. & Grieg, E. I. Over 30 years of standardized bird counts at supplementary feeding stations in north america: A citizen science data report for project FeederWatch. Frontiers in Ecology and Evolution 31, (2021).\n\n\n2. Wong, K. How birds evolved their incredible diversity. Scientific American (2020).\n\n\n3. Wilsey, C. et al. Survival by degrees: 389 bird species on the brink. (2019)."
  },
  {
    "objectID": "drop_boxes.html",
    "href": "drop_boxes.html",
    "title": "Ballot Drop Boxes",
    "section": "",
    "text": "Election season is upon us already, and I was super pumped to participate in the new vote by mail program in Baltimore! If you haven’t heard about mail-in voting, check out more election-related info here. If you’ve already received your mail-in ballot, then you might know that you can return your ballot by mail (just drop it in any USPS box), or if you’d like to save the City a little money, you can return your ballot to any of the 34 drop boxes around Baltimore City. But where exactly are these boxes, you ask? The addresses are posted online, but as a newbie to this town, it wasn’t immediately obvious to me which one was the closest…so I made a map. I hope it helps make your voting experience a little easier too. Just be sure to sign your ballot and return it by May 14, 2024, at 8p (but sooner is even better!).\n\n\n\nDrop box locations\n\n\n\n\nShow the code\n# Make pretty table\nlat_longs_pretty &lt;- lat_longs %&gt;%\n  select(Name = name,\n         Address = addr) \ndatatable(lat_longs_pretty)\n\n\n\n\n\n\nNote: this map was made using Eli Pousson’s mapbaltimore and maplayer R packages"
  },
  {
    "objectID": "drop_boxes.html#where-can-i-drop-off-my-baltimore-ballot",
    "href": "drop_boxes.html#where-can-i-drop-off-my-baltimore-ballot",
    "title": "Ballot Drop Boxes",
    "section": "",
    "text": "Election season is upon us already, and I was super pumped to participate in the new vote by mail program in Baltimore! If you haven’t heard about mail-in voting, check out more election-related info here. If you’ve already received your mail-in ballot, then you might know that you can return your ballot by mail (just drop it in any USPS box), or if you’d like to save the City a little money, you can return your ballot to any of the 34 drop boxes around Baltimore City. But where exactly are these boxes, you ask? The addresses are posted online, but as a newbie to this town, it wasn’t immediately obvious to me which one was the closest…so I made a map. I hope it helps make your voting experience a little easier too. Just be sure to sign your ballot and return it by May 14, 2024, at 8p (but sooner is even better!).\n\n\n\nDrop box locations\n\n\n\n\nShow the code\n# Make pretty table\nlat_longs_pretty &lt;- lat_longs %&gt;%\n  select(Name = name,\n         Address = addr) \ndatatable(lat_longs_pretty)\n\n\n\n\n\n\nNote: this map was made using Eli Pousson’s mapbaltimore and maplayer R packages"
  },
  {
    "objectID": "posts/analysis.html",
    "href": "posts/analysis.html",
    "title": "Example analysis",
    "section": "",
    "text": "The dark-eyed junco was the most commonly observed species in Project FeederWatch in winter 2020-21.  Photo credit: Bob Vuxinic, submitted to Project FeederWatch\n\n\n\nProject aims\nIn this example analysis, I investigate patterns in abundance and distribution of birds in the 2020-21 winter in the United States and southern Canada. Specifically, I aim to answer the following questions:\n\nwhen and where are birds observed\nwhich species are most common, and where are they found\nwhich species tend to occur in large flocks, and where are they found\n\nThis analysis is intended for the general public, as well as the citizen scientists (and the graders for Biostat 777).\n\n\nData from Project FeederWatch\nThese data were discovered via TidyTuesday for January 10, 2023, but originally come from Project FeederWatch. Project FeederWatch is a citizen science project that aims to engage individuals in North America to count birds (for as long or as little as they like) to track winter trends in bird distribution and abundance. This project has been running for more than 30 years at this point!1 The data dictionary is available here.\n\n\nShow the code\n# Load data and do quick exploration/clean-up\n\n# List required files to download from TidyTuesday\nrds_files &lt;- c(\"feederwatch.RDS\",\"site_data.RDS\")\n\n# Check if any of these files don't exist\nif (any(!file.exists(here(\"data\", rds_files)))) {\n  dir.create(here(\"data\"))\n# if missing, download the data\nfeederwatch &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-01-10/PFW_2021_public.csv')\nsite_data &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-01-10/PFW_count_site_data_public_2021.csv')\n\n# save the data objects as RDS files\nsaveRDS(feederwatch, file = here(\"data\",\"feederwatch.RDS\"))\nsaveRDS(site_data, file = here(\"data\",\"site_data.RDS\"))\n}\n\n# Load data\nfeederwatch &lt;- readRDS(here(\"data\",\"feederwatch.RDS\"))\nfeederwatch_og &lt;- feederwatch\nsite_data &lt;- readRDS(here(\"data\",\"site_data.RDS\"))\nsite_data_og &lt;- site_data\n# had to save the species_codes dictionary manually from https://docs.google.com/spreadsheets/d/1kHmx2XhA2MJtEyTNMpwqTQEnoa9M7Il2/edit#gid=2040245914\nspecies_codes_dd &lt;- read_csv(here(\"data\",\"FeederWatch_Data_Dictionary_Species_Codes.csv\"),skip = 1)\n\n\n# Quick data exploration, clean up, and creation of a few useful data frames \n# glimpse(feederwatch)\n# summary(feederwatch)\n#length(unique(feederwatch$species_code))\n\n# Change format for names in data dictionary:\nspecies_codes_dd$PRIMARY_COM_NAME &lt;- str_to_title(species_codes_dd$PRIMARY_COM_NAME)\n\n# Check for outliers in space\nlow_lat &lt;- which(feederwatch$latitude&lt;0)   # length=1; this point appears to be erroneous - remove it below\nhigh_lat &lt;- which(feederwatch$latitude&gt;50) # these are not erroneous, but want to narrow to US for plotting later on\n\n# Get rid of the one low-latitude observation\nfeederwatch &lt;- feederwatch[-low_lat,]\n# hist(feederwatch$latitude)\n\n# Add latitudinal bands to the data\nfeederwatch_df &lt;- feederwatch %&gt;%\n  mutate(Date = ymd(paste(Year, Month, Day, sep=\"-\")),\n         lat_band = case_when(\n           latitude &lt; 35 ~ \"low\",\n           latitude &gt;=35 & latitude &lt; 45 ~ \"mid\",\n           latitude &gt;= 45 ~ \"high\"\n         )) \nfeederwatch_df$lat_band &lt;- factor(feederwatch_df$lat_band, levels=c(\"high\",\"mid\",\"low\"))\n\n# Make useful dataframes:\n# Total bird count per day \nbird_count_per_day &lt;- feederwatch_df %&gt;%\n  group_by(Date, \n           lat_band) %&gt;%\n  summarize(total = sum(how_many),\n            count = n(),\n            std_total = total/count)\n\n# Abundant birds: birds that have highest counts in the dataset - i.e. flocking birds\nabundant_birds &lt;- feederwatch_df %&gt;%\n  filter(latitude&lt;50) %&gt;%\n  group_by(species_code) %&gt;%\n  summarize(total = sum(how_many),\n            count = n(),\n            std_total = total/count) %&gt;%\n  arrange(desc(std_total)) %&gt;%\n  head(n=9) %&gt;%\n  left_join(species_codes_dd, \n            by=join_by(species_code==SPECIES_CODE)) %&gt;%\n  select(species_code, \n         PRIMARY_COM_NAME, \n         SCI_NAME, count)\n\n# join with species info:\nabundant_birds_df &lt;- feederwatch_df %&gt;%\n   filter(species_code %in% abundant_birds$species_code) %&gt;%\n   left_join(species_codes_dd, \n             by=join_by(species_code==SPECIES_CODE)) %&gt;%\n   mutate(Month=factor(Month, levels=c(\"11\",\"12\",\"1\",\"2\",\"3\",\"4\")))\n\n\n# Common birds: birds that have several records in the dataset\ncommonly_sighted_birds &lt;- feederwatch_df %&gt;%\n  group_by(species_code) %&gt;%\n  summarize(total = sum(how_many),\n            count = n(),\n            std_total = total/count) %&gt;%\n  arrange(desc(count)) %&gt;%\n  head(n=9) %&gt;%\n  left_join(species_codes_dd, \n            by=join_by(species_code==SPECIES_CODE)) %&gt;%\n  select(species_code, \n         PRIMARY_COM_NAME, \n         SCI_NAME, \n         count)\n\n# join with species info:\ncommonly_sighted_birds_df &lt;- feederwatch_df %&gt;%\n   filter(species_code %in% commonly_sighted_birds$species_code) %&gt;%\n   left_join(species_codes_dd, \n             by=join_by(species_code==SPECIES_CODE)) %&gt;%\n   mutate(Month=factor(Month, levels=c(\"11\",\"12\",\"1\",\"2\",\"3\",\"4\")))\n\n\n\n\nWhere are birds observed?\nIn order to understand trends in bird distribution and abundance, we first need to know where our observations are coming from. Here is a figure showing the locations of observations. The points are transparent, so denser bird-watching areas are shaded darker green.\n\n\nShow the code\n# Figure 1: Map of unique sighting locations\n\n# Prep maps of America/USA for making figures\namerica_map &lt;- map_data(\"world\", region='USA')\nUSA_map &lt;-\n  ggplot(america_map, \n         aes(x=long, y=lat, group=group)) +\n  geom_polygon(fill=\"white\",color=\"black\") + \n  scale_x_continuous(limits = c(-125,-65)) +\n  scale_y_continuous(limits = c(25, 50)) + \n  coord_map() +\n  theme_minimal()\n\n# Identify unique sighting locations\nunique_sighting_locations &lt;- feederwatch %&gt;%\n  group_by(loc_id) %&gt;%\n  summarize(n_obs = n(),\n            latitude=mean(latitude),\n            longitude=mean(longitude))\n\n# Plot unique sighting locations\nUSA_map +\n  geom_point(data=unique_sighting_locations, \n             aes(x = longitude, y = latitude, group=NULL), \n             alpha=0.1, \n             color=\"darkgreen\") +\n  labs(x=\"Longitude (ºE)\", \n       y = \"Latitude (ºN)\", \n       title = \"Observing locations for 2020-21 winter in US and southern Canada\",\n       subtitle = \"Engagement in Project Feederwatch is stronger in the eastern than the western U.S., \\nand the highest density of birdwatchers are in the mid-Atlantic to Northeast regions.\",\n       caption = \"Data: Project FeederWatch\")\n\n\n\n\n\n\n\n\nFigure 1: Observing locations for Project FeederWatch 2020-21 winter.\n\n\n\n\n\n\n\n\n\n\n\nBirdwatching locations\n\n\n\nParticipants can birdwatch anywhere – it doesn’t have to be at their backyard feeder! But remember the distribution of observing locations in this figure since it will be relevant for understanding the distribution of common and flocking birds examined below.\n\n\nSince most bird-watching happens in the mid-latitudes (defined here as 35-45ºN), we must standardize the number of birds by the number of observations. The following figure indicates that birds are more abundant at higher latitudes even though there are fewer total observations. However, birds become more common at lower latitudes (&lt;35ºN) from January to March. This may be explained by migration to southern regions during the coldest months of the year. The fact that more birds are reported within every latitudinal band during the coldest months may indicate that more birds visit feeders to obtain food during the harshest conditions of the year.\n\n\n\n\n\nShow the code\n# Figure 2: Bird count per latitude band\nbird_count_per_day %&gt;%\n  ggplot(aes(x=Date, y = std_total, color = lat_band)) +   \n  geom_smooth() +   # loess-smoother is default\n  theme_minimal() +   \n  labs(y = \"Average number of birds per observation\", \n       title = \"Birds observed per latitude band during 2020-21 winter\", \n       color = \"Latitude band\", \n       subtitle = \"Birds are most abundant at high latitudes, but become relatively \\nmore frequent in low latitudes in the winter\", \n       caption = \"Data: Project FeederWatch\")\n\n\n\n\n\n\n\n\nFigure 2: Total number of birds observed within each latitudinal band (high: &gt;=45ºN, mid: 35-45ºN, low: &lt;35ºN).\n\n\n\n\n\n\n\nLOESS (locally weighted running line smoother) was used on these curves. This is the default option for geom_smooth.\n\n\n\nLet’s now explore how often different species are observed. A total of 361 unique species were reported, but most of these species were only observed once! However, the most common species were observed thousands of times! This is probably explained in part because birds are incredibly diverse,2 so some of these species may actually be rare, while others have may have limited distributions or be harder to identify.\n\n\nShow the code\n# Figure 3: Histogram of bird observations per species\nfeederwatch_df %&gt;%\n  group_by(species_code) %&gt;%\n  summarize(n_obs = n()) %&gt;%\n  ggplot() +\n  geom_histogram(aes(n_obs), \n                 boundary=0, \n                 closed=\"left\") +\n  labs(x = \"Number of observations for a given species\", \n       y = \"Frequency\", \n       title = \"Number of observations per bird species\",\n       subtitle = \"Most bird species are only observed once. A few common species are observed \\nthousands of times.\",\n       caption = \"Data: ProjectFeederwatch\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 3: Histogram of the number of observations per species.\n\n\n\n\n\n\n\n\n\n\n\nRare birds are becoming rarer\n\n\n\nIn 2019, the National Audubon Society estimated that two-thirds of North American birds were at risk of extinction due to climate change.3\n\n\n\n\nWhich birds are most common?\nYou’ll probably recognize the names of some of the most common birds! These birds were identified as “common” in the data set because they had the most observations (though not necessarily the highest total count of individuals).\n\n\nShow the code\n# Table 1\ncommonly_sighted_birds_table &lt;- commonly_sighted_birds %&gt;%\n  select('Common name' = PRIMARY_COM_NAME,\n         'Scientific name' = SCI_NAME,\n         'Number of observations' = count)\ncommonly_sighted_birds_table\n\n\n\nMost commonly observed birds in the 2020-21 winter in Project FeederWatch.\n\n\n\n\n\n\n\nCommon name\nScientific name\nNumber of observations\n\n\n\n\nDark-Eyed Junco\nJunco hyemalis\n5465\n\n\nDowny Woodpecker\nPicoides pubescens\n5449\n\n\nNorthern Cardinal\nCardinalis cardinalis\n5438\n\n\nHouse Finch\nHaemorhous mexicanus\n4955\n\n\nMourning Dove\nZenaida macroura\n4938\n\n\nBlack-Capped Chickadee\nPoecile atricapillus\n4696\n\n\nBlue Jay\nCyanocitta cristata\n4578\n\n\nAmerican Goldfinch\nSpinus tristis\n4446\n\n\nWhite-Breasted Nuthatch\nSitta carolinensis\n4393\n\n\n\n\n\nMost of the common birds were observed throughout the U.S. (or at least where people were reportedly birdwatching, see Figure 1). However, some birds had more limited distributions. For example, the blue jay and the northern cardinal were found predominantly in the eastern half of the U.S., and the black-capped chickadee was only observed in the more northern latitudes.\n\n\nShow the code\n# Figure 4: Commonly sighted birds\n# Since this figure takes a long time to render, we'll just load in the saved figure below, but this is the code that was used to generate it:\n  \ncommon_birds_plot &lt;- USA_map +\n  geom_bin_2d(data = commonly_sighted_birds_df, \n              aes(x = longitude, y = latitude, group=NULL)) +\n  scale_fill_continuous(low=\"lightskyblue1\", \n                        high=\"indianred4\", \n                        name=\"Number of \\nsightings\") +\n  theme_minimal() + \n  labs(x=\"Longitude (E)\", \n       y=\"Latitude (N)\", \n       title=\"Observations of common birds in the US and southern Canada\", \n       subtitle=\"Common bird species are most frequently observed in the Mid-Atlantic and \\nNortheast regions, likely due (in part) to a sampling bias and easy bird identification\", \n       color=\"Month\") + \n  facet_wrap(vars(PRIMARY_COM_NAME), \n             labeller = labeller(PRIMARY_COM_NAME = label_wrap_gen(width = 10)))\n#ggsave(here(\"figs\",\"Commonly_sighted_birds.png\"), plot=common_birds_plot, width=7, height=7, units=\"in\")\n\n\n\n\n\n\n\n\nFigure 4: Heatmap of observations for the most common birds.\n\n\n\n\n\nWhich birds are usually found in a flock?\nHave you heard the saying that “birds of a feather flock together”? The birds in the figure below had the highest average abundance per observation. This indicates to me that these birds tend to appear most often in large flocks.\n\n\nShow the code\n# Figure 5: Most abundant birds (i.e. probably largest flocks)\n# focus on birds in US (or below 50oN):\nmonth_colors = c(\"sandybrown\",\"indianred\",\"slateblue3\",\"royalblue\",\"palegreen3\")\n\nUSA_map +\n  geom_point(data = abundant_birds_df, \n             aes(x = longitude, y = latitude, group=NULL, size=how_many, color=Month), \n             alpha=0.5) +\n  scale_color_manual(values=month_colors) +   \n  theme_minimal() + \n  labs(x=\"Longitude (E)\", \n       y=\"Latitude (N)\", \n       title=\"Flocking birds observed in the US and southern Canada\", \n       subtitle=\"Birds with the highest average count per observation \\ntend to have limited distributions\", \n       caption = \"Data: Project FeederWatch\",\n       size=\"Count per observation\") +\n  facet_wrap(vars(PRIMARY_COM_NAME), \n             labeller = labeller(PRIMARY_COM_NAME = label_wrap_gen(width = 10)))\n\n\n\n\n\n\n\n\nFigure 5: Month and location for bird species with the largest average count per observation. The size of the circle is relative to the flock size, and the color indicates the month when the observation was made.\n\n\n\n\n\n\n\nSummary\nIn this example analysis, we explored patterns in the distribution and abundance of birds observed through the citizen science initiative Project FeederWatch conducted in winter 2020-21. We found that most of the observations for this program were made in the mid-Atlantic and northeast regions of the U.S. This pattern of observation corresponded strongly with the distribution of several of the most common birds, including the American goldfinch, dark-eyed junco, downy woodpecker, and house finch. However, flocking birds tended to have smaller ranges but a larger number of birds reported per observation. In general, birds were most abundant in the coldest months (January-March) and at the highest latitudes (&gt;45ºN).\n\n\nFunctions used\ndplyr: mutate, group_by, summarize, arrange, filter\nggplot2: geom_smooth, geom_histogram, geom_polygon, geom_bin_2d, geom_point, facet_wrap\n\n\n\n\n\nReferences\n\n1. Bonter, D. N. & Grieg, E. I. Over 30 years of standardized bird counts at supplementary feeding stations in north america: A citizen science data report for project FeederWatch. Frontiers in Ecology and Evolution 31, (2021).\n\n\n2. Wong, K. How birds evolved their incredible diversity. Scientific American (2020).\n\n\n3. Wilsey, C. et al. Survival by degrees: 389 bird species on the brink. (2019)."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Coming soon."
  },
  {
    "objectID": "teaching.html#philosophy",
    "href": "teaching.html#philosophy",
    "title": "Teaching",
    "section": "",
    "text": "Coming soon."
  },
  {
    "objectID": "teaching.html#duties",
    "href": "teaching.html#duties",
    "title": "Teaching",
    "section": "Duties",
    "text": "Duties\nAs a lead teaching assistant for the 620 series, I conducted laboratory sessions for up to ~50 students, during which I introduced new statistical concepts and reviewed topics previously covered in lecture. I tried to make my lectures as participatory as possible, often asking and answering questions throughout my presentation. As a teaching assistant, I held office hours (up to 5 hours per week). I also graded and provided feedback on students’ homework. By TA’ing for the 620 series and a summer workshop, I have become familiar with Stata and gained experience teaching MPH students the statistical concepts listed on the curriculum for PH 321 (e.g. statistical inference, regression, correlation, and hypothesis testing)."
  },
  {
    "objectID": "teaching.html#courses",
    "href": "teaching.html#courses",
    "title": "Teaching",
    "section": "Courses",
    "text": "Courses\nLead Teaching Assistant\n- 140.621 Statistical Methods in Public Health I (Aug - Oct 2024)\n- 140.623 Statistical Methods in Public Health III (Jan - Mar 2025)\nTeaching Assistant\n- 140.614 Data Analysis Workshop II (June 2023)\n- 140.776 Statistical Computing (Aug - Oct 2023)\n- 140.622 Statistical Methods in Public Health II (Oct - Dec 2023)\n- 140.623 Statistical Methods in Public Health III (Jan - Mar 2024)\n- 140.624 Statistical Methods in Public Health IV (Mar - May 2024)"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Coming soon."
  },
  {
    "objectID": "research.html#areas-of-research",
    "href": "research.html#areas-of-research",
    "title": "Research",
    "section": "",
    "text": "Coming soon."
  },
  {
    "objectID": "research.html#papers-in-progress",
    "href": "research.html#papers-in-progress",
    "title": "Research",
    "section": "Papers in Progress",
    "text": "Papers in Progress\nComing soon."
  },
  {
    "objectID": "research.html#presentations",
    "href": "research.html#presentations",
    "title": "Research",
    "section": "Presentations",
    "text": "Presentations\n\nNorton, E.L., Ahearn, T., Mukopadhyay, S., Balasubramanian, J., Kim, E., Pal Choudhury, P., Garcia-Closas, M., and Chatterjee, N. Modular and interpretable multicancer risk prediction of the 14 most common cancers in the U.S. to identify high-risk individuals missed by current age-based screening guidelines. inHealth Precision Medicine Symposium, Baltimore, MD. May 2025. Poster presentation.\n\nBrantley, K.D., Ahearn T., Norton, E.L., Palmer, J., Zirpoli, G., Neuhouser, M.L., Barnett, M., Teras, L., Hodge, J., Rohan, T., Milne, R., Eliassen, A.H., Huang, H., Chen, Y., O’Brien, K., Kitahara, C., Anderson, G., Lee, I, Chatterjee, N., Garcia-Closas, M., Kraft, P., on behalf of Breast Cancer Risk Prediction Project. Performance of common general-population breast cancer risk prediction models in 15 cohorts. American Association for Cancer Research (AACR) Annual Meeting, Chicago, IL. April 2025. Poster presentation: Abstract #1325."
  },
  {
    "objectID": "research.html#awards",
    "href": "research.html#awards",
    "title": "Research",
    "section": "Awards",
    "text": "Awards\nCarol Eliasberg Martin Scholarship (July 2025 - June 2026)\nInnovation in Cancer Informatics Grant Recipient (June 2025 - May 2026)"
  },
  {
    "objectID": "research.html#community-invovlement-and-leadership",
    "href": "research.html#community-invovlement-and-leadership",
    "title": "Research",
    "section": "Community Invovlement and Leadership",
    "text": "Community Invovlement and Leadership\nBiostatistics Student Organization (BSO) Mentorship Committee Co-Lead (March 2025 - present)\nBiostatistics Department Retreat Social Committee & Activity Lead (September 2024)\nBSO PhD Student Representative (August 2024 - May 2025)\nBiostatistics Department TA Training Planning Committee (June - August 2024)\nENAR Fostering Diversity in Biostatistics Workshop Committee (March 2024)\nBSO Peer Mentor (August 2023 - present)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Emily L. Norton",
    "section": "",
    "text": "I am a 4th year PhD candidate in the Department of Biostatistics in the Bloomberg School of Public Health at Johns Hopkins University. My research focuses on understanding how genetics and environmental factors influence disease risk, especially cancer, and how this risk can be predicted equitably across populations."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Education and training\n\nB.A. in biology (math minor) from Bowdoin College\nM.S. in oceanography from the University of Hawaii at Manoa\nCurrently pursuing a PhD in biostatistics at Johns Hopkins University\n\n\n\nProfessional interests\nPrior to returning for my PhD, I worked for several years doing oceanographic research. During that time, I used mathematical and statistical models to explore biological phenomena, but I wanted to better understand the fundamental principles unpinning these methods. When COVID-19 hit, I decided to pursue statistics through a public health lens.\nAs researchers in the field of public health, we have a lot of data available to us, and more is being collected everyday. However, these data don’t equally represent all communities. I’m interested in how we can make sense of big data, and ways we can fill in missing data, to improve people’s quality of life in an equitable way. More specifically, my research focuses on understanding how genetics and environmental factors influence disease risk, and how this risk changes across different populations. I hope my research will help inform patients and doctors about ways to tailor preventive screenings to individuals who need it most, in order to prevent cancer and efficiently use limited resources.\n\n\nPersonal interests\nIn my free time, I enjoy hiking, foraging, crafting, and baking.\n\n\n\nForaging for mushrooms in the Cascade Mountains in Washington. Photo credit: Austin Voecks"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "Example analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]