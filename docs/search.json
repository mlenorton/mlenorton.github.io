[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "Population Attributable Fraction (PAF)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Epidemiological Quantities\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBallot Drop Boxes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem Demo with Polygenic Risk Scores\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is risk?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Education and training\n\nB.A. in biology (math minor) from Bowdoin College\nM.S. in oceanography from the University of Hawaii at Manoa\nCurrently pursuing a PhD in biostatistics at Johns Hopkins University\n\n\n\nProfessional interests\nPrior to returning for my PhD, I worked for several years doing oceanographic research. During that time, I used mathematical and statistical models to explore biological phenomena, but I wanted to better understand the fundamental principles unpinning these methods. When COVID-19 hit, I decided to pursue statistics through a public health lens.\nAs researchers in the field of public health, we have a lot of data available to us, and more is being collected everyday. However, these data don’t equally represent all communities. I’m interested in how we can make sense of big data, and ways we can fill in missing data, to improve people’s quality of life in an equitable way. More specifically, my research focuses on understanding how genetics and environmental factors influence disease risk, and how this risk changes across different populations. I hope my research will help inform patients and doctors about ways to tailor preventive screenings to individuals who need it most, in order to prevent cancer and efficiently use limited resources.\n\n\nPersonal interests\nIn my free time, I enjoy hiking, foraging, crafting, and baking.\n\n\n\nForaging for mushrooms in the Cascade Mountains in Washington. Photo credit: Austin Voecks"
  },
  {
    "objectID": "posts/risk.html",
    "href": "posts/risk.html",
    "title": "What is risk?",
    "section": "",
    "text": "Imagine that you go to your doctor. You’d like to know your (individual!) risk for some cancer. Let’s talk about what this actually means. Your risk is defined as the probability that you will actually get that disease. The concept of “true risk” is debated because you either will develop cancer or you won’t1 (see the “problem of the single-case” below). For now, we assume that this is not pre-determined. This estimate for your true risk is practical because 1) neither you, the best researchers, nor the smartest clinicians can be sure that you definitely will or will not develop cancer, and 2) it may prompt you to take preventive behaviors, such as increased screenings or changing your modifiable risk factors (e.g. diet and exercise), that could help reduce your risk.\nAssuming that a true risk does exist, let’s talk about your hypothetical probability of developing cancer. We’ll let this little blue circle represent a healthy version of you:\n\n\n\n\n\n\nMy goal as a researcher is to estimate your (individual) probability of getting cancer. In a statistically-perfect, fictitious universe, I would study many (let’s say 100) counterfactual versions of you. These counterfactuals would be identical to you in every way. Let me repeat: in each world, all known and unknown characteristics that make you you, would be replicated perfectly.\n\n\n\n\n\n\nThen, in each world, I would monitor you to see if you develop cancer. For this demonstration, let’s say you develop cancer in 12 out of 100 worlds. We would thus estimate that your probability of developing cancer is \\(\\frac{12}{100} * 100\\% = 12\\%\\).\n\n\n\n\n\n\nLet’s stop right there. First of all - why do we get different results in different worlds? As an empiricist, I find this apparent lack of reproducibility deeply unintuitive, even troubling. Secondly, did you notice that I estimated a probability as an observed proportion - why did I do that? Thirdly, once we have an estimate for your probability of disease, what does it even mean? Finally, in the real world, we can’t study 100 counterfactual versions of you, so what do we do? This discussion aims to address each of these points (at somewhat inconsistent levels of depth). Peruse various sections depending on your interests."
  },
  {
    "objectID": "posts/risk.html#the-set-up",
    "href": "posts/risk.html#the-set-up",
    "title": "What is risk?",
    "section": "",
    "text": "Imagine that you go to your doctor. You’d like to know your (individual!) risk for some cancer. Let’s talk about what this actually means. Your risk is defined as the probability that you will actually get that disease. The concept of “true risk” is debated because you either will develop cancer or you won’t1 (see the “problem of the single-case” below). For now, we assume that this is not pre-determined. This estimate for your true risk is practical because 1) neither you, the best researchers, nor the smartest clinicians can be sure that you definitely will or will not develop cancer, and 2) it may prompt you to take preventive behaviors, such as increased screenings or changing your modifiable risk factors (e.g. diet and exercise), that could help reduce your risk.\nAssuming that a true risk does exist, let’s talk about your hypothetical probability of developing cancer. We’ll let this little blue circle represent a healthy version of you:\n\n\n\n\n\n\nMy goal as a researcher is to estimate your (individual) probability of getting cancer. In a statistically-perfect, fictitious universe, I would study many (let’s say 100) counterfactual versions of you. These counterfactuals would be identical to you in every way. Let me repeat: in each world, all known and unknown characteristics that make you you, would be replicated perfectly.\n\n\n\n\n\n\nThen, in each world, I would monitor you to see if you develop cancer. For this demonstration, let’s say you develop cancer in 12 out of 100 worlds. We would thus estimate that your probability of developing cancer is \\(\\frac{12}{100} * 100\\% = 12\\%\\).\n\n\n\n\n\n\nLet’s stop right there. First of all - why do we get different results in different worlds? As an empiricist, I find this apparent lack of reproducibility deeply unintuitive, even troubling. Secondly, did you notice that I estimated a probability as an observed proportion - why did I do that? Thirdly, once we have an estimate for your probability of disease, what does it even mean? Finally, in the real world, we can’t study 100 counterfactual versions of you, so what do we do? This discussion aims to address each of these points (at somewhat inconsistent levels of depth). Peruse various sections depending on your interests."
  },
  {
    "objectID": "posts/risk.html#what-is-randomness-and-where-does-it-come-from",
    "href": "posts/risk.html#what-is-randomness-and-where-does-it-come-from",
    "title": "What is risk?",
    "section": "2. What is randomness and where does it come from?",
    "text": "2. What is randomness and where does it come from?\nTrue randomness is the absolute lack of pattern or predictability of events, and it is arguably an intrinsic aspect of natural systems2:\n\nQuantum mechanics states that we cannot precisely know both the position and velocity of subatomic particles, and by measuring either, we increase the uncertainty around the other quantity. Practically, this means that we cannot predict radioactive decay times for individual atoms, Brownian motion, heat transfer patterns, or cosmic radiation, to name a few. Thus, quantum mechanics serves as an inherent source of randomness in the universe.\n\nAlternatively, apparent randomness can be debunked by taking a closer look at the system, upon which order, pattern, and/or predictability could be discerned:\n\nChaos is unpredictability of a system arising due to high sensitivity to initial conditions. Given the slightest change in subatomic or atomic factors, we can see wide variations in outcomes (i.e. the butterfly effect).\nStochasticity is the idea that a very large number (i.e. an uncountable number) of external agents interact, creating an extremely complex web of factors that affect an outcome.\n\nApparently random processes are extremely complex. Sometimes we approximate them with true randomness to simplify our model and minimize required computational power. For example, genetic mutations and environmental exposures (e.g. air pollution, smoking, diet, exercise, UV exposure, etc.) may not be truly random3, but we can simplify our calculations dramatically by selecting only the most important risk factors and grouping the others into a random error term."
  },
  {
    "objectID": "posts/risk.html#why-do-we-use-a-proportion-to-estimate-a-probability",
    "href": "posts/risk.html#why-do-we-use-a-proportion-to-estimate-a-probability",
    "title": "What is risk?",
    "section": "3. Why do we use a proportion to estimate a probability?",
    "text": "3. Why do we use a proportion to estimate a probability?\nThe field of statistics gives us several tools to estimate a parameter, which is exactly what our probability is here. Let’s begin by re-stating our problem mathematically. We will consider your health status (yes, yours!) as a random variable since we don’t know your outcome yet. Your possible outcomes are healthy (denoted as a \\(0\\)) and diseased (denoted as a \\(1\\)). Thus, we aim to estimate your probability of being diseased, \\(p\\), defined as \\(p = P(X = 1)\\), where \\(P\\) is a probability function. Thus, since we have two possible outcomes, and one probability parameter, we will assume that \\(X\\) follows a Bernoulli distribution with probability p:\n\\[X \\sim Ber(p)\\]\n\nA. Method of moments estimation\nThe method of moments estimation technique is based on the result from the Weak Law of Large Numbers (WLLN), which says that the empirical mean \\(\\bar{X}\\) approaches the true mean of a random variable [X] as the number of observations \\(n\\) gets large. More formally, we state the WLLN as follows:\n\\[\n\\bar{X_n} \\to^p \\mathbb{E}[X]\n\\] where \\(\\to^p\\) indicates convergence in probability.\nThus, we can substitute the observed empirical mean \\(\\bar{x}\\) for its expectation, and solve for our parameter \\(p\\) as follows:\n\\[\n\\begin{align}\n\\mathop{\\mathbb{E}}[X] &= p \\\\\n\\bar{X} &\\approx p \\\\  \n\\hat{p} &= \\frac{1}{n}\\sum_{i=1}^{n}X_i\n\\end{align}\n\\]\n\n\nB. Maximum likelihood estimation\nAn alternative approach, that has many appealing properties, is known as maximum likelihood estimation, or MLE. This method reframes the problem in such as way that we can think about finding the parameter \\(p\\) that makes our observations \\(x\\) most likely. For a \\(Ber(p)\\) random variable, the likelihood function, which is the same as the joint probability mass function, is\n\\[\n\\begin{align}\n{\\cal{L}}{(p|X)} &= \\Pi_{i=1}^n P(X_i|p)  \\\\\n&= \\Pi_{i=1}^{n} p^{X_i}(1-p)^{1 - X_i}\n\\end{align}\n\\]\nWe will maximize the log likelihood (since it is easier to work with), by taking its derivative, setting it equal to zero, and solving for \\(p\\):\n\\[\n\\begin{align}\nlog({\\cal{L}}) &\\propto \\sum_{i=1}^{n} X_i log(p) + (1-X_i)log(1-p) \\\\\n\\frac{dlog{\\cal{L}}}{dp} &\\propto \\frac{\\sum_{i=1}^{n}{X_i}}{p} - \\frac{n-\\sum_{i=1}^{n}{X_i}}{1-p}  \\\\\n0 &= \\frac{\\sum_{i=1}^{n}{X_i}}{p} - \\frac{n-\\sum_{i=1}^{n}{X_i}}{1-p}  \\\\\n\\hat{p} &= \\frac{1}{n}\\sum_{i=1}^{n}{X_i}\n\\end{align}\n\\]\nIn summary, both methods give the same estimate for \\(p\\) which is equal to the total number of successes (or in this case, the number of times you develop the disease: 12) divided by the total number of trials (or in this case, the total number of fictitious worlds where we followed a counterfactual version of you: n = 100). Thus, we arrive at our previous estimate that you have a 12% probability of getting cancer."
  },
  {
    "objectID": "posts/risk.html#what-is-a-probability-anyway",
    "href": "posts/risk.html#what-is-a-probability-anyway",
    "title": "What is risk?",
    "section": "4. What is a probability anyway?",
    "text": "4. What is a probability anyway?\nNow that I’ve told you that your true risk of developing cancer is 12% - so what?? What are some ways you can interpret that statement?\nIntuitively, many of us think about probability as the chance or likelihood of something happening. More rigorously, recall that a probability measure must meet the three axioms stated by Kolmogorov (1933). Simply put, they say that 1) the total probability of all possible outcomes must equal 1, 2) for any possible outcome, its probability must be between 0 and 1 (i.e. no negative probabilities), and 3) that the probability of mutually exclusive outcomes is equal to the sum of the probabilities of each outcome. Mathematically, we write:\nFor probability triple, \\((\\Omega, \\cal{F}, P)\\), where \\(\\Omega\\) = sample space, \\(\\cal{F}\\) = \\(\\sigma\\)-algebra, and \\(P\\) = probability measure, \\[P(\\Omega) = 1, \\] \\[P(p) \\ge 0 \\text{ for any } p \\in \\Omega,\\] \\[P(\\cup_{i} p_i) = \\Sigma_{i} p_i \\text{ for countable disjoint } p_i.\\]\nThere are at least six distinct ways to interpret probabilities4, but I will highlight the three most common. I won’t go into each thoroughly, but I want to give a sense for the range of what a probability might mean. Perhaps the similarities and differences between the interpretations will help us understand the concept of probability more deeply. I think it’s important for the person communicating the probability as well as the one receiving it to be aware of the potential ways that the information may be interpreted.\n\nA. Classical probability\nCore idea: Probability is determined a priori by identifying all possible outcomes and assigning each one an equal chance.\nExample: Assume a coin is fair. Then since there are two sides (heads and tails), we attribute equal probability to each: \\(P(heads) = P(tails) = 0.5\\).\nBreakdown: Large (uncountable) probability spaces, spaces with unknown/unaccounted outcomes, unequal chances for different events (e.g. weighted dice, unfair coin)\n\n\n\nHeads and tails of a U.S. quarter\n\n\n\n\nB. Frequency interpretations\nCore idea: Frequentists assume that the long-run frequency of events is fixed, objective, and intrinsically tied to the probability of that event. Thus, instead of considering all of the possible outcomes (as in classical probability), we will use the observed outcomes and compare this to the expected results assuming a null hypothesis4.\nExample: Let’s assess whether a coin is fair by estimating the probability of a coin landing on heads. We would flip the coin many times and estimate the probability by counting the number of heads and dividing it by the number of total trials. If this is (sufficiently) close to 0.5, we would assume this is a fair coin. Experiments with a large number of repeated trials or large sample sizes of independent events are good applications for frequentists statistics.\nBreakdown: Suppose we only observed a few of the trials above. If we observed up to 10 trials (first row below) we would be tempted to conclude that your cancer risk is 0/10 = 0%. However, if we observed 11 trials, we would conclude that your risk is 1/11 = 9.1%. With 50 trials, we would estimate that your cancer risk is 4/50 = 8%. But none of these is correct. We know theoretically that a sequence of unlikely events is possible, but frequentists assume that the probability that corresponds most closely with the observed data, regardless of prior knowledge, is the best estimate. As the number of trials increases, this becomes less of a problem, but few real-life events are repeatable an arbitrary number of times, and many are downright unrepeatable (e.g. the Big Bang, volcanic eruption/earthquake/natural disaster, 2024 presidential election, etc.) leaving us with a gap in how to reconcile the true probability of the event. This is known as the “problem of the single-case.” This also becomes much more complex in the event of dependent events.\n\n\n\n\n\n\n\n\nC. Subjective interpretation (aka Bayesian)\nCore idea: A probability represents someone’s degree of belief, confidence, or credence. This “someone” is a rational and logically consistent “suitable agent” who follows the axioms of probability. However, these credences are inherently subjective (hence the name). Your level of credence about an event can be estimated by what you would consider a “fair bet” (i.e. putting your money where you mouth is): “Your degree of belief in E is p iff p units of utility is the price at which you would buy or sell a bet that pays 1 unit of utility if E, 0 if not E”4. In other words, you are indifferent to placing a bet \\(p\\) on \\(E\\) as you are to placing \\(1-p\\) on \\(\\neg E\\). In the statistical framework, someone’s belief is based on prior knowledge - summarized as a prior probability with a selected distribution and parameter(s) - which is combined with observed data to obtain a posterior, from which conclusions are drawn5.\n\\(\\text{Bayes Rule:}\\) \\[\nP(H|E) = \\frac{P(E|H) \\cdot P(H)}{P(E)}\n\\] where \\(H\\) is the hypothesis, \\(P(H)\\) is the estimate of the probability of the hypothesis (i.e. “prior” probability), \\(E\\) is the evidence (i.e. data), \\(P(E)\\) is the marginal likelihood of the data, \\(P(E|H)\\) is the “likelihood” of the data given the fixed hypothesis, and \\(P(H|E)\\) is the probability of the hypothesis given the fixed data (i.e. “posterior” probability).\nExample: We could evaluate the hypothesis that a coin is fair compared to the hypothesis that it is not by calculating the posterior under each prior. First we might assume that the coin flip is fair (i.e. the flip is distributed Bernoulli(0.5)). Then we flip the coin many times and count the number of heads. We then estimate the likelihood of this event given the fixed hypothesis that the coin is fair. Finally, we multiply the likelihood and the prior to generate a posterior probability that the coin is fair given the data. It is common to compare two hypotheses, so we may also repeat the process assuming another parameter for the prior probability. Then we could compare the posterior probabilities for these two hypotheses and see which is more probable.\nBreakdown: Selecting a prior is subjective and there are few well-defined methods for choosing one6. Sometimes there is minimal prior knowledge, in which case a uniform prior may be used; this is the same as doing a frequentist analysis.\nGoing back to our cancer example, we employed a frequentist approach to estimate your risk of cancer (in some fictitious universe). If we had a priori knowledge about your risk of cancer, e.g. given your family history, age, or other factors, we could update our probability estimate by combining the observed data and this prior knowledge in a Bayesian approach."
  },
  {
    "objectID": "posts/risk.html#how-do-we-actually-estimate-your-probability-of-disease",
    "href": "posts/risk.html#how-do-we-actually-estimate-your-probability-of-disease",
    "title": "What is risk?",
    "section": "5. How do we actually estimate your probability of disease?",
    "text": "5. How do we actually estimate your probability of disease?\nIn reality, of course, we can’t study 100 counterfactual versions of you. We have to make an estimate with what’s available to us now. Since you are a human, let’s estimate the risk within a group of 100 randomly sampled humans:\n\n\n\n\n\n\nAfter following these 100 individuals, we find that 11 people get cancer. Risk (or cumulative incidence) is estimated as the number of incident (i.e. new) cases within a given time divided by the number of individuals in the study population (sounds familiar, eh?). Thus, we estimate an 11% risk within this study population.\n\n\n\n\n\n\nThen, assuming that all individuals in this study population are independent and identically distributed*, using the math above, we assign each individual within the target population (which includes you!) the same probability of disease. * The assumption of the participants being identically distributed is critical, and impossible to test. Practically, researchers and clinicians attempt to group individuals with similar risk factors into the same group to estimate risk, but defining which population is most relevant to you is challenging if not impossible."
  },
  {
    "objectID": "posts/risk.html#the-reference-class-problem",
    "href": "posts/risk.html#the-reference-class-problem",
    "title": "What is risk?",
    "section": "6. The reference class problem",
    "text": "6. The reference class problem\nFor demonstration purposes, let’s assume the same overall risk of 11% in the total population. Now consider the subset of this population that are men (i.e. the left half of the population): 8/50 men get sick, so their risk is 16%. Women (on the right), however, get sick at a rate of 3/50 = 6%. We can subdivide our population further by age, or any other risk factors (or any combination of these characteristics), and we estimate different risks for each subpopulation. Taking this to an extreme, we see that a young man has a 20% risk, and an older man has a 12% risk; a young woman has an 8% risk, and an older woman has a 4% risk. So which population - and consequently which risk estimate - is most relevant for a given individual?\n\n\n\n\n\n\nThis predicament of identifying the appropriate population for which to estimate risk is known as the “reference class problem.” This problem was identified by John Venn in 1876: “every single thing or event has an indefinite number of properties or attributes observable in it, and might therefore be considered as belonging to an indefinite number of different classes of things”7. Thus, Stern (2012)1 argues that there is no such thing as unconditional or individual risk. The risk factors included in the model that we select determine the population classes, and thus the risk estimates for each class. Even for well-calibrated models, when we compare two models, the risk estimates associated with any one individual can vary substantially depending on which reference class an individual is assigned to8."
  },
  {
    "objectID": "posts/risk.html#limitations",
    "href": "posts/risk.html#limitations",
    "title": "What is risk?",
    "section": "7. Limitations",
    "text": "7. Limitations\nThe goal of precision medicine is to make the reference class as small as possible so that there is less variability within that group, and better estimates of risk are assigned to each individual9. This is a tricky problem and is discussed thoroughly by Kent et al. (2018). Most importantly, risk should be clearly communicated to doctors and patients that risk is estimated using a specific model, and “people with X, Y, and Z risk factors have a W% risk for disease Q.”\nIn my research, however, we skirt the individual risk issue by making predictions at the population-level. We can feel confident making these predictions when the model is shown to perform well on a given population (i.e. it is “well-calibrated”). Thus, our models can help inform how cancer screening guidelines should set to screen the top X percentage of the population at highest risk, or other allocations of resources.\nAdditional limitations include the problem of adjusting someone’s risk if their covariates change over time - it is currently unknown how this will affect risk since most models only include covariates measured at baseline. Issues surrounding generalizability or transportability to other populations for groups that are under-represented in study populations are also common."
  },
  {
    "objectID": "posts/drop_boxes.html",
    "href": "posts/drop_boxes.html",
    "title": "Ballot Drop Boxes",
    "section": "",
    "text": "Election season is upon us already, and I was super pumped to participate in the new vote by mail program in Baltimore! If you haven’t heard about mail-in voting, check out more election-related info here. If you’ve already received your mail-in ballot, then you might know that you can return your ballot by mail (just drop it in any USPS box), or if you’d like to save the City a little money, you can return your ballot to any of the 34 drop boxes around Baltimore City. But where exactly are these boxes, you ask? The addresses are posted online, but as a newbie to this town, it wasn’t immediately obvious to me which one was the closest…so I made a map. I hope it helps make your voting experience a little easier too. Just be sure to sign your ballot and return it by May 14, 2024, at 8p (but sooner is even better!).\n\n\n\nDrop box locations\n\n\n\n\nShow the code\n# Make pretty table\nlat_longs_pretty &lt;- lat_longs %&gt;%\n  select(Name = name,\n         Address = addr) \ndatatable(lat_longs_pretty)\n\n\n\n\n\n\nNote: this map was made using Eli Pousson’s mapbaltimore and maplayer R packages"
  },
  {
    "objectID": "posts/drop_boxes.html#where-can-i-drop-off-my-baltimore-ballot",
    "href": "posts/drop_boxes.html#where-can-i-drop-off-my-baltimore-ballot",
    "title": "Ballot Drop Boxes",
    "section": "",
    "text": "Election season is upon us already, and I was super pumped to participate in the new vote by mail program in Baltimore! If you haven’t heard about mail-in voting, check out more election-related info here. If you’ve already received your mail-in ballot, then you might know that you can return your ballot by mail (just drop it in any USPS box), or if you’d like to save the City a little money, you can return your ballot to any of the 34 drop boxes around Baltimore City. But where exactly are these boxes, you ask? The addresses are posted online, but as a newbie to this town, it wasn’t immediately obvious to me which one was the closest…so I made a map. I hope it helps make your voting experience a little easier too. Just be sure to sign your ballot and return it by May 14, 2024, at 8p (but sooner is even better!).\n\n\n\nDrop box locations\n\n\n\n\nShow the code\n# Make pretty table\nlat_longs_pretty &lt;- lat_longs %&gt;%\n  select(Name = name,\n         Address = addr) \ndatatable(lat_longs_pretty)\n\n\n\n\n\n\nNote: this map was made using Eli Pousson’s mapbaltimore and maplayer R packages"
  },
  {
    "objectID": "posts/PAF.html",
    "href": "posts/PAF.html",
    "title": "Population Attributable Fraction (PAF)",
    "section": "",
    "text": "In the last section, we discussed individual risk and realized that it was really hard to estimate! Fortunately, we found that population-level estimates were much easier to make. We also discussed how risk estimates were based on a model with specific covariates comprising a set of risk factors and potential confounding variables. In this page, we ask questions surrounding how risk for a disease would change at a population level if a risk factor were eliminated - e.g. imagine a world where no one smokes, everyone drinks clean water, we all exercise regularly, etc. Let’s explore this concept called “population attributable fraction” or PAF.\n\n\nShow the code\n# Turn off axes\nNoax &lt;- list(    \n  title = \"\",\n  zeroline = FALSE,\n  showline = FALSE,\n  showticklabels = FALSE,\n  showgrid = FALSE\n)\n\n# Set up vector for diseased individuals to be colored differently\nN &lt;- 100\ndiseased_inds &lt;- union(union(seq(8,100,10),seq(9,100,10)),seq(10,100,10))\ndiseased_vec &lt;- vector(mode = \"character\", length = 100)\ndiseased_vec[diseased_inds] &lt;- 'rgb(255, 65, 54)'\ndiseased_vec[-diseased_inds] &lt;- 'rgb(66, 209, 209)'\nrisk_factor_inds &lt;- c(1:20)\nrisk_factor_vec &lt;- vector(mode = \"character\", length = 100)\nrisk_factor_vec[diseased_inds] &lt;- 'rgb(255, 65, 54)'\nrisk_factor_vec[-diseased_inds] &lt;- 'rgb(66, 209, 209)'\nrisk_factor_vec[risk_factor_inds] &lt;- 'rgb(0,0,0)'  #'rgb(145, 0, 0)'\n\n\n# Set up text vector\nz_text &lt;- vector(mode = \"character\", length = 100)\nz_text[diseased_inds] &lt;- \"sick\"\nz_text[-diseased_inds] &lt;- \"healthy\"\nz_text[risk_factor_inds] &lt;- 'risk factor'\n\ndata_100_diseased_RF &lt;- data.frame(x = rep(seq(1,10,1), each = 10), y = rep(seq(1,10,1), times = 10), z = z_text)\n\n\nfig_100_diseased_RF &lt;- plot_ly(data_100_diseased_RF, x = ~x, y = ~y, text = ~z, type = 'scatter', mode = 'markers', \n        marker = list(size = 20, \n                      opacity = 0.7, \n                      color = diseased_vec, \n                      line = list(color = risk_factor_vec,\n                                  width = 3)),\n        hoverinfo = 'text')   # TODO: don't show location; change color\n\nfig_100_diseased_RF &lt;- fig_100_diseased_RF %&gt;% \n  layout(title = 'What proportion of this disease is attributable to the risk factor?',\n         xaxis = Noax,\n         yaxis = Noax) %&gt;%\n  config(displayModeBar = FALSE)\n\nfig_100_diseased_RF"
  },
  {
    "objectID": "posts/PAF.html#the-set-up",
    "href": "posts/PAF.html#the-set-up",
    "title": "Population Attributable Fraction (PAF)",
    "section": "",
    "text": "In the last section, we discussed individual risk and realized that it was really hard to estimate! Fortunately, we found that population-level estimates were much easier to make. We also discussed how risk estimates were based on a model with specific covariates comprising a set of risk factors and potential confounding variables. In this page, we ask questions surrounding how risk for a disease would change at a population level if a risk factor were eliminated - e.g. imagine a world where no one smokes, everyone drinks clean water, we all exercise regularly, etc. Let’s explore this concept called “population attributable fraction” or PAF.\n\n\nShow the code\n# Turn off axes\nNoax &lt;- list(    \n  title = \"\",\n  zeroline = FALSE,\n  showline = FALSE,\n  showticklabels = FALSE,\n  showgrid = FALSE\n)\n\n# Set up vector for diseased individuals to be colored differently\nN &lt;- 100\ndiseased_inds &lt;- union(union(seq(8,100,10),seq(9,100,10)),seq(10,100,10))\ndiseased_vec &lt;- vector(mode = \"character\", length = 100)\ndiseased_vec[diseased_inds] &lt;- 'rgb(255, 65, 54)'\ndiseased_vec[-diseased_inds] &lt;- 'rgb(66, 209, 209)'\nrisk_factor_inds &lt;- c(1:20)\nrisk_factor_vec &lt;- vector(mode = \"character\", length = 100)\nrisk_factor_vec[diseased_inds] &lt;- 'rgb(255, 65, 54)'\nrisk_factor_vec[-diseased_inds] &lt;- 'rgb(66, 209, 209)'\nrisk_factor_vec[risk_factor_inds] &lt;- 'rgb(0,0,0)'  #'rgb(145, 0, 0)'\n\n\n# Set up text vector\nz_text &lt;- vector(mode = \"character\", length = 100)\nz_text[diseased_inds] &lt;- \"sick\"\nz_text[-diseased_inds] &lt;- \"healthy\"\nz_text[risk_factor_inds] &lt;- 'risk factor'\n\ndata_100_diseased_RF &lt;- data.frame(x = rep(seq(1,10,1), each = 10), y = rep(seq(1,10,1), times = 10), z = z_text)\n\n\nfig_100_diseased_RF &lt;- plot_ly(data_100_diseased_RF, x = ~x, y = ~y, text = ~z, type = 'scatter', mode = 'markers', \n        marker = list(size = 20, \n                      opacity = 0.7, \n                      color = diseased_vec, \n                      line = list(color = risk_factor_vec,\n                                  width = 3)),\n        hoverinfo = 'text')   # TODO: don't show location; change color\n\nfig_100_diseased_RF &lt;- fig_100_diseased_RF %&gt;% \n  layout(title = 'What proportion of this disease is attributable to the risk factor?',\n         xaxis = Noax,\n         yaxis = Noax) %&gt;%\n  config(displayModeBar = FALSE)\n\nfig_100_diseased_RF"
  },
  {
    "objectID": "posts/PAF.html#what-is-population-attributable-fraction",
    "href": "posts/PAF.html#what-is-population-attributable-fraction",
    "title": "Population Attributable Fraction (PAF)",
    "section": "2. What is population attributable fraction?",
    "text": "2. What is population attributable fraction?\n\nBackground\nDeveloped by Levin (1953),1 the concept of population attributable fraction (PAF) was originally used to estimate the impact of smoking on lung cancer risk. PAF is known by many names, including “population attributable risk”, “population attributable risk proportion”, “population attributable risk percentage”, “etiologic fraction”, “excess fraction”, or sometimes just “attributable risk”.2,3\n\n\nMathematical definition\n\\[\nPAF = \\frac{\\Pr(E)(RR - 1)}{\\Pr(E)(RR-1) + 1}\n\\tag{1}\\] where \\(\\Pr(E)\\) is the probability of exposure and \\(RR = \\frac{\\Pr(D | E)}{\\Pr(D | \\bar{E})}\\) is the relative risk.\n\n\nInterpretations\nThe PAF has two main interpretations. The first is the fractional reduction in average disease risk in a population for a given time interval that would be achieved by eliminating a (known disease-causing) exposure while all other risk factors stayed the same. A slightly more intuitive interpretation is the proportion of disease cases that would be prevented in a population during a specified time interval if no one were exposed to a given risk factor.3\n\n\nAssumptions\nThe fundamental assumptions of PAF are:1 4\n\nCausation: The exposure (aka risk factor) causes the disease.\nStable Unit Treatment Values (SUTVA): No interference between exposures and no hidden variations of exposure.\n\nNo confounding: We assume that the unexposed and exposed groups do not have underlying differences other than their exposure status. Thus, if an individual who is exposed had not been, they would have the same risk as an unexposed individual.\n\n\n\nUtility\nPAF is an epidemiologic measure of the potential impact a public health intervention could have on a disease by eliminating a specific risk factor.5 It is important to remember that PAF is the proportion that the disease burden could be reduced - not a direct measure of the number of cases that could be prevented."
  },
  {
    "objectID": "posts/PAF.html#formulae-for-paf",
    "href": "posts/PAF.html#formulae-for-paf",
    "title": "Population Attributable Fraction (PAF)",
    "section": "3. Formulae for PAF",
    "text": "3. Formulae for PAF\nNow we will investigate several common equations used to calculate PAF (see derivations in Section 8). I have attempted to put them in common terms to aid understanding and cross-connections between them. Also, I have estimated the PAF for three disease/risk factor pairs where possible to help orient the reader and to demonstrate how the formulae may differ: BRCA mutation and breast cancer, smoking and lung cancer, and poor air quality and mortality.2,6–11\n\nOriginal equation\nThe original formula was developed by Levin (1953)1 in a paper that discussed the relationship between smoking and lung cancer during a time in history when the causal relationship was just starting to take shape. Levin developed an index \\(S\\) to quantify the “indicated [proportion] of all lung cancer attributable to smoking.” The index, now known as the PAF, was derived as follows:\n\\[\\begin{align}\nS &= \\frac{\\text{incidence of lung cancer attributable to smoking}}{\\text{total incidence of lung cancer}} \\notag \\\\\n&= \\frac{rXb \\frac{r-1}{r}}{rXb + X(1-b)} \\notag \\\\\n&= \\frac{rb\\frac{r-1}{r}}{rb + (1-b)} \\notag \\\\\n&= \\frac{b(r-1)}{b(r-1) + 1}\n\\end{align}\\]\nwhere \\[\\begin{align*}\nX &= \\text{incidence of lung cancer in non-smokers} \\\\\nrX &= \\text{incidence of lung cancer in smokers} \\\\\nb &= \\text{proportion of general population classified as smokers} \\\\\n1-b &= \\text{proportion of general population classified as non-smokers}\n\\end{align*}\\]\nThus, a few useful quantities can be defined: \\[\\begin{align*}\nrXb + X(1-b) &= \\text{incidence of lung cancer in the total population} \\\\\nr = \\frac{rX}{X} &= \\frac{\\text{incidence of lung cancer in smokers}}{\\text{incidence of lung cancer in non-smokers}} \\\\\n\\frac{r-1}{r} &= \\text{proportion of lung cancer in smokers attributable to smoking}\n\\end{align*}\\]\nNotice that these are all estimable quantities (i.e. statistics) rather than theoretical quantities. However, we can easily translate the final expression to the theoretical quantities underpinning this index, and generalize them as follows:12\n\\[\nPAF = \\frac{\\Pr(E)(RR - 1)}{\\Pr(E)(RR-1) + 1}\n\\] where \\(\\Pr(E)\\) is the probability of exposure and \\(RR = \\frac{\\Pr(D | E)}{\\Pr(D | \\bar{E})}\\) is the relative risk.\nThis quantity can be estimated using observable quantities:\n\\[\n\\widehat{PAF} = \\frac{(\\text{prevalence of the exposure})(\\widehat{RR} - 1)}{(\\text{prevalence of the exposure})(\\widehat{RR} - 1) + 1}\n\\] where \\(\\widehat{RR} = \\frac{\\text{incidence in exposed group}}{\\text{incidence in unexposed group}}\\), usually estimated in a prospective cohort study or approximated by an odds ratio derived from a case-control study when a disease is rare. The prevalence of an exposure is often estimated using cross-sectional studies.\n\n\nShow the code\n# Build example disease dataframe\n\ndeaths_air_quality &lt;- data.frame(disease = \"mortality\", exposure = \"poor air quality\", RR = 1.062, Prev_exp = 1, rate_pop = 1, rate_unexp = 1, pd = 1, num_prev_cases = 3279857)  # European Env Agency: https://www.eea.europa.eu/publications/assessing-the-risks-to-health#:~:text=Relative%20risks%20capture%20the%20increase,be%20assigned%20to%20specific%20individuals; num_prev_cases = total mortality in US in 2022 (https://www.cdc.gov/nchs/fastats/deaths.htm)\nbreast_cancer_BRCA &lt;- data.frame(disease = \"breast cancer\", exposure = \"BRCA mutation\", RR = 30, Prev_exp = 0.0025, rate_pop = 0.13, rate_unexp = 0.1261, pd = 0.03, num_prev_cases = 240000)  # Table 3 (and Discussion), Antoniou et al., 2003; Nelson et al., 2013 - risk in women; rate_pop = lifetime incidence of BC = 13% in US women, and 3% of cancers are in women with BRCA mutation; num_prev_cases = breast cancer diagnosed per year in women in US,\nlung_cancer_smoking &lt;- data.frame(disease = \"lung cancer\", exposure = \"smoking\", RR = 7, Prev_exp = 0.12, rate_pop = 0.0625, rate_unexp = 0.0125, pd = 0.85, num_prev_cases = 238300) # RR: O'Keeffe et al., 2018; Prev_exp: CDC Fast Facts; rate_pop: SEER*Stat; pd: NCI: https://www.cancer.gov/news-events/press-releases/2021/lung-cancer-never-smokers#:~:text=Lung%20cancer%20is%20the%20leading,lung%20cancer%20have%20never%20smoked; rate_pop = lifetime incidence of lung cancer = 1/16, 20% of which occur in non-smokers; num_prev_cases =  estimated number of lung cancer cases each year (see calc: \n# 47,660 non-smokers diagnosed with lung cancer each year, accounting for 20% of lung cancer cases (https://www.lungcancerresearchfoundation.org/lung-cancer-facts/?gad_source=1&gclid=CjwKCAjwl4yyBhAgEiwADSEjePEa6HgTX7PzQYb8SNjQNxA6RIU4jvzLIr5VY7pT7uihX02s9tEI4hoCZBUQAvD_BwE)\n# total_lung &lt;- 47660/0.2  # --&gt; 238300 total lung cancer cases each year?? \n\nexample_diseases &lt;- rbind(deaths_air_quality,breast_cancer_BRCA, lung_cancer_smoking) %&gt;%\n  mutate(descr = paste0(disease, \" due to \\n\", exposure))\n\n\n\n\nShow the code\n# Equation 1\nRR &lt;- 1:35\nPrev_exp &lt;- seq(0,1,0.01)\n\ncalc_PAF_eqn1 &lt;- function(RR, Prev_exp) {\n  PAF &lt;- (Prev_exp*(RR - 1))/(1 + Prev_exp*(RR-1))\n  return(PAF)}\n\npaf_df_eqn1 &lt;- data.frame(RR = rep(RR, times = length(Prev_exp)), Prev_exp = rep(Prev_exp, each = length(RR)))\n\n# Calculate PAF for example diseases\nexample_diseases &lt;- example_diseases %&gt;%\n  mutate(PAF_eqn1 = calc_PAF_eqn1(RR,Prev_exp))\nexample_diseases$x_anno_1 &lt;- c(5.5, 29, 10)\nexample_diseases$y_anno_1 &lt;- c(0.94, 0.1, 0.17)\n\n# Make contour plot with points for example diseases\npaf_df_eqn1 %&gt;%\n  ggplot(aes(RR, Prev_exp, z = calc_PAF_eqn1(RR,Prev_exp))) +\n  geom_contour_filled() +\n  geom_point(data = example_diseases, shape = 21, size = 3, color = \"black\",  fill = \"white\") +\n  annotate(\"text\", x = example_diseases$x_anno_1, y = example_diseases$y_anno_1, label = example_diseases$descr, color = \"white\") +\n  labs(x = \"Relative risk\", y = \"Prevalence of exposure\", title = \"PAF: Original formula\", fill = \"PAF\") +\n  theme_minimal() +\n  scale_fill_viridis(discrete=TRUE, guide = guide_legend(reverse=TRUE))\n\n\n\n\n\n\n\n\n\n\n\nCommon alternative formula\nUsing Bayes Theorem and some algebra, we can alternatively express the original equation in another commonly used form:3\n\\[\nPAF = \\frac{\\Pr(D) - \\Pr(D|\\bar{E})}{\\Pr(D)}\n\\tag{2}\\]\nwhere \\(\\Pr(D)\\) is the probability of disease in a population which may contain unexposed and exposed individuals, and \\(\\Pr(D|\\bar{E})\\) is the hypothetical probability of disease in that same population if no one were exposed to the risk factor. This quantity can be estimated using observable incidence rates, assuming the unexposed population is representative of the entire population:\n\\[\n\\widehat{PAF} =  \\frac{\\text{Incidence}_{\\text{population}} - \\text{Incidence}_{\\text{unexposed}}}{\\text{Incidence}_{\\text{population}}}\n\\]\n\n\nShow the code\n# Equation 2\nrate_pop &lt;- seq(0,1,0.01)\nrate_unexp &lt;- seq(0,1,0.01)\n\ncalc_PAF_eqn2 &lt;- function(rate_pop, rate_unexp) {\n  PAF &lt;- (rate_pop - rate_unexp)/rate_pop\n  return(PAF)\n}\n\npaf_df_eqn2 &lt;- data.frame(rate_pop = rep(rate_pop,times = length(rate_unexp)), rate_unexp = rep(rate_unexp, each=length(rate_pop)))\n\nmy_breaks &lt;- c(-Inf, seq(0,1,0.1))\n\n# Calculate PAF for example diseases\nexample_diseases &lt;- example_diseases %&gt;%\n  mutate(PAF_eqn2 = calc_PAF_eqn2(rate_pop,rate_unexp))\nexample_diseases$x_anno_2 &lt;- c(0.88, 0.2, 0.16)\nexample_diseases$y_anno_2 &lt;- c(0.94, 0.22, 0.05)\n\npaf_df_eqn2 %&gt;%\n  ggplot(aes(rate_pop, rate_unexp, z = calc_PAF_eqn2(rate_pop,rate_unexp))) +\n  geom_contour_filled(breaks = my_breaks) +\n  geom_point(data = example_diseases, shape = 21, size = 3, color = \"black\",  fill = \"white\") +\n  annotate(\"text\", x = example_diseases$x_anno_2, y = example_diseases$y_anno_2, label = example_diseases$descr, color = \"white\") +\n  labs(x = \"Incidence in population\", y = \"Incidence in unexposed\", title = \"PAF: Common alternative formula\", fill = \"PAF\") +\n  theme_minimal() +\n  scale_fill_viridis(discrete=TRUE, guide = guide_legend(reverse=TRUE))\n\n\n\n\n\n\n\n\n\n\n\nRosen’s formula\nRosen (2013) aimed to develop a more “intuitive” formula to estimate \\(PAF\\), which relies on only two quantities:5\n\\[\n\\widehat{PAF} = \\frac{A}{N_d}\n\\tag{3}\\] where \\(A\\) is the number of people who developed the disease because of the exposure and \\(N_d\\) is the total number of people with the disease. While this is a more intuitive formulation, \\(A\\) cannot be estimated directly.\n\n\nShow the code\n# Equation 3 (Rosen et al., 2013)\n\n# Total number of people with the disease\nN_d &lt;- 1:1000  # roughly the population of the US\n\n# People who developed the disease because of the exposure\nA &lt;- 1:1000   # roughly the population of the US\n\ncalc_PAF_eqn3 &lt;- function(A,N_d) {\n  PAF &lt;- A/N_d\n  return(PAF)\n}\n\npaf_df_eqn3 &lt;- data.frame(x = rep(N_d,times = length(A)), y = rep(A, each=length(N_d)))\n\npaf_df_eqn3 %&gt;%\n  ggplot(aes(x, y, z = calc_PAF_eqn3(y,x))) +\n geom_contour_filled(breaks = my_breaks) +\n  labs(x = \"Total number of cases\", y = \"Number of cases due to exposure\", title = \"PAF: Rosen's formula\", fill = \"PAF\") +\n  theme_minimal() +\n  scale_fill_viridis(discrete=TRUE, guide = guide_legend(reverse=TRUE)) \n\n\n\n\n\n\n\n\n\n\n\nMiettinen’s formula\nAnother expression developed by Miettinen (1974) relies on the prevalence of the exposure only within the diseased population:13\n\\[\nPAF = \\Pr(E|D) \\left( \\frac{RR - 1}{RR} \\right)\n\\tag{4}\\] where \\(\\Pr(E|D)\\) is the probability of exposure among the cases, and \\(RR\\) is relative risk. Thus,\n\\[\n\\widehat{PAF} = (\\text{proportion of cases who were exposed}) \\times \\left( \\frac{\\widehat{RR} - 1}{\\widehat{RR}} \\right)\n\\]\n\n\nShow the code\n# Equation 4\n# Relative risk\nRR &lt;- 1:35\n  \n# Proportion of cases exposed to risk factor\npd &lt;- seq(0,1,0.01)\n\ncalc_PAF_eqn4 &lt;- function(pd,RR) {\n  PAF &lt;- (pd*(RR - 1))/(RR)\n  return(PAF)\n}\n\npaf_df_eqn4 &lt;- data.frame(RR = rep(RR,times = length(pd)), pd = rep(pd, each=length(RR)))\n\n\nexample_diseases &lt;- example_diseases %&gt;%\n  mutate(PAF_eqn4 = calc_PAF_eqn4(pd,RR))\n\nexample_diseases$x_anno_4 &lt;- c(5.5, 29, 12)\nexample_diseases$y_anno_4 &lt;- c(0.94, 0.12, 0.8)\n\n  \npaf_df_eqn4 %&gt;%\n  ggplot(aes(RR, pd, z = calc_PAF_eqn4(pd,RR))) +\n  geom_contour_filled(breaks = my_breaks) + \n   geom_point(data = example_diseases, shape = 21, size = 3, color = \"black\",  fill = \"white\") +\n  annotate(\"text\", x = example_diseases$x_anno_4, y = example_diseases$y_anno_4, label = example_diseases$descr, color = \"white\") +\n  labs(x = \"Relative risk\", y = \"Proportion of cases exposed\", title = \"PAF: Miettinen's formula\", fill = \"PAF\") +\n  theme_minimal() +\n  scale_fill_viridis(discrete=TRUE, guide = guide_legend(reverse=TRUE))\n\n\n\n\n\n\n\n\n\n\n\nElasticity formula\nFinally, for people with a background in economics, it may be useful to think of PAF as the elasticity of incidence (\\(\\Pr(D)\\)) for a change in prevalence of a risk factor (\\(\\Pr(E)\\)). Elasticity here indicates how disease incidence changes, relative to a baseline incidence, for a given change in the prevalence of a risk factor, relative to a baseline prevalence for that risk factor:14\n\\[\nPAF = \\frac{\\Pr(E)}{\\Pr(D)}\\left( \\frac{d\\Pr(D)}{d\\Pr(E)} \\right)\n\\tag{5}\\]\nwhere \\[\n\\begin{align*}\n& d \\cdot = \\text{derivative} \\\\\n& \\Pr(E) = \\text{prevalence of exposure (or risk factor) in the population} \\\\\n& \\Pr(D) = \\text{incidence of disease in the population}\n\\end{align*}\n\\] This formulation is not easily represented graphically."
  },
  {
    "objectID": "posts/PAF.html#extensions-of-paf",
    "href": "posts/PAF.html#extensions-of-paf",
    "title": "Population Attributable Fraction (PAF)",
    "section": "4. Extensions of PAF",
    "text": "4. Extensions of PAF\n\nMulti-category risk factors\nPAF can also be partitioned into different levels or categories of exposure, and then summed across all (disjoint) categories to get the total PAF. This is known as the “distributive property” of the PAF.15 Equation 4 can thus be adapted to estimate category-specific PAF for multiple levels of exposure:\n\\[\nPAF_i = \\Pr(E_i|D) \\left( \\frac{RR_i - 1}{RR_i} \\right)\n\\]\nwhere \\(i\\) is the \\(i^{th}\\) disjoint category of exposure, and \\(RR_i\\) is the relative risk for the \\(i^{th}\\) category relative to the reference category, which by definition has \\(RR_0 = 0\\). By countable additivity, we can then calculate the total PAF as:\n\\[\n\\begin{align*}\nPAF &= \\sum_i PAF_i \\\\\n&= \\sum_i \\Pr(E_i|D) \\left( \\frac{RR_i - 1}{RR_i} \\right) \\\\\n&= 1 - \\sum_i \\frac{\\Pr(E_i|D)}{RR_i}\n\\end{align*}\n\\]\n\n\nConfounding and adjusted relative risks\nIf the primary assumption of no confounding is violated (see Section 2.4), then the PAF must be adjusted for confounders.4,12 Equation 4 can be modified to include adjusted relative risks that account for potential confounders:16\n\\[\nPAF_a = \\Pr(E|D,Z) \\left( \\frac{RR_a - 1}{RR_a} \\right) \\\\\n\\] where \\(Z\\) is the set of confounders that have been used to estimated the adjusted relative risk \\(\\widehat{RR_a}\\).\n\n\nGeneralized attributable fraction\nThis extension allows a different baseline distribution for the risk factor rather than assuming that it is eliminated completely:17\n\\[\nGIF = \\frac{\\Pr(D) - \\Pr(D^*)}{\\Pr(D)}\n\\]\nwhere \\(\\Pr(D)\\) and \\(\\Pr(D^*)\\) are the probability of disease under the current and modified distributions of exposure, respectively. The \\(GIF\\) allows for a more realistic real-world estimate of what proportion of diseases would be prevented if, for example, half of the overweight population reduced their BMI by a certain percent, fewer people smoked, etc.\n\n\nMultiple risk factors\nIt is commonly recognized that most diseases have multiple risk factors, and if at least one could be eliminated, the disease would not develop. Thus, when you want to calculate \\(PAF\\) for several risk factors, it is best to calculate \\(PAF\\) when all of the risk factors are considered simultaneously, otherwise you could erroneously sum \\(PAF\\) for individual risk factors to obtain a total \\(PAF &gt; 1\\).3 The only conditions where the total \\(PAF = \\sum_i PAF_i\\) are when 1) no individuals are exposed to multiple risk factors, or 2) when the risks factors have an additive effect on disease incidence.4\n\n\nVariance estimation\nVariance can be estimated using the Delta method, and recent advances with bootstrapping and jackknifing have been made.12"
  },
  {
    "objectID": "posts/PAF.html#caution",
    "href": "posts/PAF.html#caution",
    "title": "Population Attributable Fraction (PAF)",
    "section": "5. Caution",
    "text": "5. Caution\nAlthough there are relatively few assumptions required in the theoretical PAF, it is often challenging to estimate the it in practice. For example, rarely does one study estimate all of the necessary quantities, so studies often synthesize estimates from different sources. Under this scenario, portability of results becomes a real issue.18 Researchers must be vigilant to ensure that the definition of the exposure and outcome is consistent and that the relative risks are estimated without confounding, or in the case of adjusted relative risks, that the same covariates were adjusted for across studies.\nFrom a utility standpoint, PAF is most useful to estimate for a risk factor that is modifiable.3 For example, calculating the PAF for a disease based on a risk factor of age, sex, race/ethnicity, genetics, other unchangeable quantities, or diagnostic results is not very helpful for preventing the disease."
  },
  {
    "objectID": "posts/PAF.html#summary-and-conclusions",
    "href": "posts/PAF.html#summary-and-conclusions",
    "title": "Population Attributable Fraction (PAF)",
    "section": "6. Summary and Conclusions",
    "text": "6. Summary and Conclusions\nPAF is a useful index to estimate the proportion of cases attributable to a specific risk factor. PAF has many common formulae. Although these equations are relatively simple and rely on few theoretical assumptions, researchers must be diligent in implementing them carefully using estimable quantities, often gleaned from diverse sources. Extensions have been developed to accommodate scenarios where multiple exposure levels exist or to deal with confounding by using adjusted relative risk estimates. Since PAF is a proportion, both common diseases with a low PAF or rare diseases with a high PAF could have a significant number of cases prevented by a public health intervention. Thus, in some situations, its useful to estimate the absolute number of preventable cases by multiplying the PAF by the number of prevalent cases.\n\n\nShow the code\n# TODO: can we make the plots interactive so that we get the PAF when we hover over?\nPAF &lt;- seq(0,1,0.01)\nnum_prev_cases &lt;- seq(1,4000000,1000)\n\ncalc_abs_lives &lt;- function(PAF, Prev_disease) {\n  abs_lives &lt;- PAF * Prev_disease\n}\n\nabs_lives_df &lt;- data.frame(PAF = rep(PAF, times = length(num_prev_cases)), num_prev_cases = rep(num_prev_cases, each=length(PAF)))\n\n# Calculate abs cases prevented for example diseases\nexample_diseases &lt;- example_diseases %&gt;%\n  mutate(PAF = PAF_eqn1)\nexample_diseases$x_anno_cases &lt;- c(0.2, 0.2, 0.55)\nexample_diseases$y_anno_cases &lt;- c(3010857, 500000, 400000)\n\nmy_breaks &lt;- c(0,200000,seq(500000,4000000,500000))\n\nabs_lives_df %&gt;%\n  ggplot(aes(PAF, num_prev_cases, z = calc_abs_lives(PAF,num_prev_cases))) +\n geom_contour_filled(breaks = my_breaks) +\n  geom_point(data = example_diseases, shape = 21, size = 3, color = \"black\",  fill = \"white\") +\n  annotate(\"text\", x = example_diseases$x_anno_cases, y = example_diseases$y_anno_cases, label = example_diseases$descr, color = \"white\") +\n  labs(x = \"Population attributable fraction\", y = \"Number of prevalent cases\", title = \"Absolute number of cases prevented\", fill = \"Cases prevented\") +\n  theme_minimal() +\n  scale_fill_batlow(discrete=TRUE, guide=guide_legend(reverse=TRUE))"
  },
  {
    "objectID": "posts/PAF.html#acknowledgements",
    "href": "posts/PAF.html#acknowledgements",
    "title": "Population Attributable Fraction (PAF)",
    "section": "7. Acknowledgements",
    "text": "7. Acknowledgements\nThanks to Grace Ringlein, Ciprian Crainiceanu, and Nilanjan Chatterjee for discussion and feedback on this project."
  },
  {
    "objectID": "posts/PAF.html#sec-derivations",
    "href": "posts/PAF.html#sec-derivations",
    "title": "Population Attributable Fraction (PAF)",
    "section": "8. Derivations",
    "text": "8. Derivations\nEquation 2\n\\[\\begin{align*}\nPAF &= \\frac{\\Pr(E)(RR - 1)}{\\Pr(E)(RR-1) + 1} \\\\\n&= \\frac{\\Pr(E)(\\frac{\\Pr(D|E)}{\\Pr(D|\\bar{E})} - 1)}{\\Pr(E)(\\frac{\\Pr(D|E)}{\\Pr(D|\\bar{E})}-1) + 1} \\\\\n&= \\frac{\\frac{\\Pr(D,E)}{\\Pr(D|\\bar{E})} - \\Pr(E)}{\\frac{\\Pr(D,E)}{\\Pr(D|\\bar{E})} - \\Pr(E) + 1} \\times \\frac{\\Pr(D|\\bar{E})}{\\Pr(D|\\bar{E})}\\\\\n&= \\frac{\\Pr(D,E) - \\Pr(D|\\bar{E}) \\Pr(E)}{\\Pr(D,E) - \\Pr(D|\\bar{E}) \\Pr(E) + \\Pr(D|\\bar{E})}  \\\\\n&= \\frac{\\Pr(D,E) - \\Pr(D|\\bar{E}) (1-\\Pr(\\bar{E}))}{\\Pr(D,E) - \\Pr(D|\\bar{E}) (1-\\Pr(\\bar{E})) + \\Pr(D|\\bar{E})}  \\\\\n&= \\frac{\\Pr(D,E) - \\Pr(D|\\bar{E}) +\\Pr(D,\\bar{E})}{\\Pr(D,E) - \\Pr(D|\\bar{E}) + \\Pr(D,\\bar{E}) + \\Pr(D|\\bar{E})}  \\\\\n\n&= \\frac{\\Pr(D) - \\Pr(D|\\bar{E})}{\\Pr(D)}\n\\end{align*}\\]\nEquation 3\n\\[\\begin{align*}\nPAF &= \\frac{\\Pr(E)(RR - 1)}{\\Pr(E)(RR-1) + 1} \\\\\n&= \\frac{\\Pr(E) \\left( \\frac{\\Pr(D|E)}{\\Pr(D|\\bar{E})} - 1 \\right)}{\\Pr(E) \\left( \\frac{\\Pr(D|E)}{\\Pr(D|\\bar{E})}-1\\right) + 1} \\times \\frac{\\Pr(D|\\bar{E})}{\\Pr(D|\\bar{E})}\\\\\n&= \\frac{\\Pr(E)(\\Pr(D|E) - \\Pr(D|\\bar{E}))}{\\Pr(E)(\\Pr(D|E) - \\Pr(D|\\bar{E})) + \\Pr(D|\\bar{E})} \\\\\n&= \\frac{\\Pr(E)(\\Pr(D|E) - \\Pr(D|\\bar{E}))}{\\Pr(E)(\\Pr(D|E) - \\Pr(D|\\bar{E})) + (1-\\Pr(E) + \\Pr(E))\\Pr(D|\\bar{E})} \\times \\frac{N}{N}\\\\\n&= \\frac{N \\Pr(E)(\\Pr(D|E) - \\Pr(D|\\bar{E}))}{N\\Pr(E)(\\Pr(D|E) - \\Pr(D|\\bar{E})) + N\\Pr(E))\\Pr(D|\\bar{E}) + N(1-\\Pr(E))\\Pr(D|\\bar{E})}\n\\end{align*}\\]\nWe can estimate this theoretical expression using observable quantities:\n\\[\\begin{align*}\n\\widehat{PAF} &= \\frac{A}{A + B + C} \\\\\n&= \\frac{A}{N_d}\n\\end{align*}\\]\nwhere \\(N\\) is the total number of individuals in the population, \\(A\\) is the number of exposed individuals whose disease can be attributed to their exposure, \\(B\\) is the number of exposed individuals whose disease is not attributed to their exposure, \\(C\\) is the number of non-exposed individuals with the disease, and \\(N_d\\) is the total number of individuals with disase.\nEquation 4\n\\[\\begin{align*}\nPAF &= \\frac{\\Pr(E)(RR - 1)}{\\Pr(E)(RR-1) + 1} \\\\\n&= \\frac{\\Pr(E)(RR - 1)}{\\Pr(E) \\left( \\frac{\\Pr(D|E)}{\\Pr(D|\\bar{E})}-1 \\right) + 1} \\times \\frac{\\Pr(D|\\bar{E})}{\\Pr(D|\\bar{E})} \\\\\n&= \\frac{\\Pr(D|\\bar{E})\\Pr(E)(RR - 1)}{\\Pr(E)(\\Pr(D|E) - \\Pr(D|\\bar{E})) + \\Pr(D|\\bar{E})} \\\\\n&= \\frac{\\Pr(D|\\bar{E})\\Pr(E)(RR - 1)}{\\Pr(D, E) - \\Pr(D|\\bar{E})) + \\Pr(D, \\bar{E}) + \\Pr(D|\\bar{E})} \\\\\n&= \\frac{\\Pr(D|\\bar{E})\\Pr(E)(RR - 1)}{\\Pr(D)} \\times \\frac{\\frac{\\Pr(D|E)}{\\Pr(D|\\bar{E})}}{\\frac{\\Pr(D|E)}{\\Pr(D|\\bar{E})}} \\\\\n&= \\frac{\\Pr(D|E)\\Pr(E)(RR - 1)}{\\Pr(D) \\frac{\\Pr(D|E)}{\\Pr(D|\\bar{E})}} \\\\\n&= \\Pr(E|D) \\left( \\frac{RR - 1}{RR} \\right)\n\\end{align*}\\]\nEquation 5\nFirst notice that disease incidence (\\(\\Pr(D)\\)) can be stated as a function of the prevalance of exposure to a risk factor (\\(\\Pr(E)\\)):\n\\[\\begin{align*}\n\\Pr(D) &= \\Pr(D,E) + \\Pr(D,\\bar{E}) \\\\\n&= \\Pr(D|E) \\Pr(E) + \\Pr(D|\\bar{E}) \\Pr(\\bar{E}) \\\\\n&= \\Pr(D|E) \\Pr(E) + \\Pr(D|\\bar{E}) (1-\\Pr(E)) \\\\\n&= \\Pr(D|\\bar{E}) + \\Pr(E) (\\Pr(D|E) - \\Pr(D|\\bar{E}))\n\\end{align*}\\]\nThen we can take the derivative of incidence with respect to prevalence of exposure:\n\\[\\begin{align*}\n\\frac{d\\Pr(D)}{d\\Pr(E)} &= \\frac{d(\\Pr(D|\\bar{E}) + \\Pr(E) (\\Pr(D|E) - \\Pr(D|\\bar{E})))}{d\\Pr(E)} \\\\\n&= \\Pr(D|E) - \\Pr(D|\\bar{E})\n\\end{align*}\\]\nThus, we can derive the elasticity equation as follows:\n\\[\\begin{align*}\nPAF &= \\frac{\\Pr(E)(RR - 1)}{\\Pr(E)(RR - 1) + 1}  \\\\\n&= \\frac{\\Pr(E)(\\frac{\\Pr(D|E)}{\\Pr(D|\\bar{E})} - 1)}{\\Pr(E)(\\frac{\\Pr(D|E)}{\\Pr(D|\\bar{E})}-1) + 1} \\times \\frac{\\Pr(D|\\bar{E})}{\\Pr(D|\\bar{E})}\\\\\n&= \\frac{\\Pr(E)(\\Pr(D|E) -\\Pr(D|\\bar{E}))}{\\Pr(E)(\\Pr(D|E) -\\Pr(D|\\bar{E})) + \\Pr(D|\\bar{E})} \\\\\n&= \\frac{\\Pr(E)(\\Pr(D|E) -\\Pr(D|\\bar{E}))}{\\Pr(D)} \\\\\n&= \\frac{\\Pr(E)}{\\Pr(D)} \\frac{d\\Pr(D)}{d\\Pr(E)} \\\\\n\\end{align*}\\]"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Coming soon."
  },
  {
    "objectID": "teaching.html#philosophy",
    "href": "teaching.html#philosophy",
    "title": "Teaching",
    "section": "",
    "text": "Coming soon."
  },
  {
    "objectID": "teaching.html#duties",
    "href": "teaching.html#duties",
    "title": "Teaching",
    "section": "Duties",
    "text": "Duties\nAs a lead teaching assistant for the 620 series, I conducted laboratory sessions for up to ~50 students, during which I introduced new statistical concepts and reviewed topics previously covered in lecture. I tried to make my lectures as participatory as possible, often asking and answering questions throughout my presentation. As a teaching assistant, I held office hours (up to 5 hours per week). I also graded and provided feedback on students’ homework. By TA’ing for the 620 series and a summer workshop, I have become familiar with Stata and gained experience teaching MPH students a wide array of statistical concepts (e.g. statistical inference, regression, correlation, and hypothesis testing)."
  },
  {
    "objectID": "teaching.html#courses",
    "href": "teaching.html#courses",
    "title": "Teaching",
    "section": "Courses",
    "text": "Courses\nLead Teaching Assistant\n- 140.621 Statistical Methods in Public Health I (Aug - Oct 2024)\n- 140.623 Statistical Methods in Public Health III (Jan - Mar 2025)\nTeaching Assistant\n- 140.614 Data Analysis Workshop II (June 2023)\n- 140.776 Statistical Computing (Aug - Oct 2023)\n- 140.622 Statistical Methods in Public Health II (Oct - Dec 2023)\n- 140.623 Statistical Methods in Public Health III (Jan - Mar 2024)\n- 140.624 Statistical Methods in Public Health IV (Mar - May 2024)"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Coming soon."
  },
  {
    "objectID": "research.html#areas-of-research",
    "href": "research.html#areas-of-research",
    "title": "Research",
    "section": "",
    "text": "Coming soon."
  },
  {
    "objectID": "research.html#papers-in-progress",
    "href": "research.html#papers-in-progress",
    "title": "Research",
    "section": "Papers in Progress",
    "text": "Papers in Progress\nComing soon."
  },
  {
    "objectID": "research.html#presentations",
    "href": "research.html#presentations",
    "title": "Research",
    "section": "Presentations",
    "text": "Presentations\n\nNorton, E.L., Ahearn, T., Mukopadhyay, S., Balasubramanian, J., Kim, E., Pal Choudhury, P., Garcia-Closas, M., and Chatterjee, N. Modular and interpretable multicancer risk prediction of the 14 most common cancers in the U.S. to identify high-risk individuals missed by current age-based screening guidelines. inHealth Precision Medicine Symposium, Baltimore, MD. May 2025. Poster presentation.\n\nBrantley, K.D., Ahearn T., Norton, E.L., Palmer, J., Zirpoli, G., Neuhouser, M.L., Barnett, M., Teras, L., Hodge, J., Rohan, T., Milne, R., Eliassen, A.H., Huang, H., Chen, Y., O’Brien, K., Kitahara, C., Anderson, G., Lee, I, Chatterjee, N., Garcia-Closas, M., Kraft, P., on behalf of Breast Cancer Risk Prediction Project. Performance of common general-population breast cancer risk prediction models in 15 cohorts. American Association for Cancer Research (AACR) Annual Meeting, Chicago, IL. April 2025. Poster presentation: Abstract #1325."
  },
  {
    "objectID": "research.html#awards",
    "href": "research.html#awards",
    "title": "Research",
    "section": "Awards",
    "text": "Awards\nCarol Eliasberg Martin Scholarship (July 2025 - June 2026)\nInnovation in Cancer Informatics Grant Recipient (June 2025 - May 2026)"
  },
  {
    "objectID": "research.html#community-invovlement-and-leadership",
    "href": "research.html#community-invovlement-and-leadership",
    "title": "Research",
    "section": "Community Invovlement and Leadership",
    "text": "Community Invovlement and Leadership\nBiostatistics Student Organization (BSO) Mentorship Committee Co-Lead (March 2025 - present)\nBiostatistics Department Retreat Social Committee & Activity Lead (September 2024)\nBSO PhD Student Representative (August 2024 - May 2025)\nBiostatistics Department TA Training Planning Committee (June - August 2024)\nENAR Fostering Diversity in Biostatistics Workshop Committee (March 2024)\nBSO Peer Mentor (August 2023 - present)"
  },
  {
    "objectID": "posts/Basic_epi_quants.html",
    "href": "posts/Basic_epi_quants.html",
    "title": "Basic Epidemiological Quantities",
    "section": "",
    "text": "Let’s define some basic concepts used in epidemiology that will form the foundation for several of our calculations on other pages. Consider a population of 100 people. Some people are sick (the red-filled dots), and some are healthy (the blue-filled dots). The people with a black outline have some risk factor for the disease.\n\n\nShow the code\n# Turn off axes\nNoax &lt;- list(    \n  title = \"\",\n  zeroline = FALSE,\n  showline = FALSE,\n  showticklabels = FALSE,\n  showgrid = FALSE\n)\n\n# Set up vector for diseased individuals to be colored differently\nN &lt;- 100\ndiseased_inds &lt;- union(union(seq(8,100,10),seq(9,100,10)),seq(10,100,10))\ndiseased_vec &lt;- vector(mode = \"character\", length = 100)\ndiseased_vec[diseased_inds] &lt;- 'rgb(255, 65, 54)'\ndiseased_vec[-diseased_inds] &lt;- 'rgb(66, 209, 209)'\nrisk_factor_inds &lt;- c(1:20)\nrisk_factor_vec &lt;- vector(mode = \"character\", length = 100)\nrisk_factor_vec[diseased_inds] &lt;- 'rgb(255, 65, 54)'\nrisk_factor_vec[-diseased_inds] &lt;- 'rgb(66, 209, 209)'\nrisk_factor_vec[risk_factor_inds] &lt;- 'rgb(0,0,0)'  #'rgb(145, 0, 0)'\n\n\n# Set up text vector\nz_text &lt;- vector(mode = \"character\", length = 100)\nz_text[diseased_inds] &lt;- \"sick\"\nz_text[-diseased_inds] &lt;- \"healthy\"\nz_text[risk_factor_inds] &lt;- 'risk factor'\n\ndata_100_diseased_RF &lt;- data.frame(x = rep(seq(1,10,1), each = 10), y = rep(seq(1,10,1), times = 10), z = z_text)\n\n\nfig_100_diseased_RF &lt;- plot_ly(data_100_diseased_RF, x = ~x, y = ~y, text = ~z, type = 'scatter', mode = 'markers', \n        marker = list(size = 20, \n                      opacity = 0.7, \n                      color = diseased_vec, \n                      line = list(color = risk_factor_vec,\n                                  width = 3)),\n        hoverinfo = 'text')   # TODO: don't show location; change color\n\nfig_100_diseased_RF &lt;- fig_100_diseased_RF %&gt;% \n  layout(title = '',\n         xaxis = Noax,\n         yaxis = Noax) %&gt;%\n  config(displayModeBar = FALSE)\n\nfig_100_diseased_RF\n\n\n\n\n\n\nNow let’s think about how this schematic might look different for different diseases."
  },
  {
    "objectID": "posts/Basic_epi_quants.html#the-set-up",
    "href": "posts/Basic_epi_quants.html#the-set-up",
    "title": "Basic Epidemiological Quantities",
    "section": "",
    "text": "Let’s define some basic concepts used in epidemiology that will form the foundation for several of our calculations on other pages. Consider a population of 100 people. Some people are sick (the red-filled dots), and some are healthy (the blue-filled dots). The people with a black outline have some risk factor for the disease.\n\n\nShow the code\n# Turn off axes\nNoax &lt;- list(    \n  title = \"\",\n  zeroline = FALSE,\n  showline = FALSE,\n  showticklabels = FALSE,\n  showgrid = FALSE\n)\n\n# Set up vector for diseased individuals to be colored differently\nN &lt;- 100\ndiseased_inds &lt;- union(union(seq(8,100,10),seq(9,100,10)),seq(10,100,10))\ndiseased_vec &lt;- vector(mode = \"character\", length = 100)\ndiseased_vec[diseased_inds] &lt;- 'rgb(255, 65, 54)'\ndiseased_vec[-diseased_inds] &lt;- 'rgb(66, 209, 209)'\nrisk_factor_inds &lt;- c(1:20)\nrisk_factor_vec &lt;- vector(mode = \"character\", length = 100)\nrisk_factor_vec[diseased_inds] &lt;- 'rgb(255, 65, 54)'\nrisk_factor_vec[-diseased_inds] &lt;- 'rgb(66, 209, 209)'\nrisk_factor_vec[risk_factor_inds] &lt;- 'rgb(0,0,0)'  #'rgb(145, 0, 0)'\n\n\n# Set up text vector\nz_text &lt;- vector(mode = \"character\", length = 100)\nz_text[diseased_inds] &lt;- \"sick\"\nz_text[-diseased_inds] &lt;- \"healthy\"\nz_text[risk_factor_inds] &lt;- 'risk factor'\n\ndata_100_diseased_RF &lt;- data.frame(x = rep(seq(1,10,1), each = 10), y = rep(seq(1,10,1), times = 10), z = z_text)\n\n\nfig_100_diseased_RF &lt;- plot_ly(data_100_diseased_RF, x = ~x, y = ~y, text = ~z, type = 'scatter', mode = 'markers', \n        marker = list(size = 20, \n                      opacity = 0.7, \n                      color = diseased_vec, \n                      line = list(color = risk_factor_vec,\n                                  width = 3)),\n        hoverinfo = 'text')   # TODO: don't show location; change color\n\nfig_100_diseased_RF &lt;- fig_100_diseased_RF %&gt;% \n  layout(title = '',\n         xaxis = Noax,\n         yaxis = Noax) %&gt;%\n  config(displayModeBar = FALSE)\n\nfig_100_diseased_RF\n\n\n\n\n\n\nNow let’s think about how this schematic might look different for different diseases."
  },
  {
    "objectID": "posts/Basic_epi_quants.html#incidence-of-disease",
    "href": "posts/Basic_epi_quants.html#incidence-of-disease",
    "title": "Basic Epidemiological Quantities",
    "section": "2. Incidence of disease",
    "text": "2. Incidence of disease\nSome diseases are much less (or more) common - this refers to their incidence. Consider incidence rates of malaria infection (~27% in Nigeria) versus influenza (~15% for the 2023 season in the US) versus breast cancer (~13% of US women over their lifetime). You might notice that these rates are time and space specific, and they vary widely between different populations. For incidence rates, and the other variables discussed below, it is important to precisely determine your population of study and your case definition, otherwise your calculations may be biased.\n\n\nShow the code\n# Set up vector for diseased individuals to be colored differently\nN &lt;- 100\ndiseased_inds &lt;- seq(10,100,10)\ndiseased_vec &lt;- vector(mode = \"character\", length = 100)\ndiseased_vec[diseased_inds] &lt;- 'rgb(255, 65, 54)'\ndiseased_vec[-diseased_inds] &lt;- 'rgb(66, 209, 209)'\nrisk_factor_inds &lt;- c(1:20)\nrisk_factor_vec &lt;- vector(mode = \"character\", length = 100)\nrisk_factor_vec[diseased_inds] &lt;- 'rgb(255, 65, 54)'\nrisk_factor_vec[-diseased_inds] &lt;- 'rgb(66, 209, 209)'\nrisk_factor_vec[risk_factor_inds] &lt;- 'rgb(0,0,0)' #'rgb(145, 0, 0)'\n\n\n# Set up text vector\nz_text &lt;- vector(mode = \"character\", length = 100)\nz_text[diseased_inds] &lt;- \"sick\"\nz_text[-diseased_inds] &lt;- \"healthy\"\nz_text[risk_factor_inds] &lt;- 'risk factor'\n\ndata_100_rare_diseased_RF &lt;- data.frame(x = rep(seq(1,10,1), each = 10), y = rep(seq(1,10,1), times = 10), z = z_text)\n\n\nfig_100_rare_diseased_RF &lt;- plot_ly(data_100_rare_diseased_RF, x = ~x, y = ~y, text = ~z, type = 'scatter', mode = 'markers', \n        marker = list(size = 20, \n                      opacity = 0.7, \n                      color = diseased_vec, \n                      line = list(color = risk_factor_vec,\n                                  width = 3)),\n        hoverinfo = 'text')   # TODO: don't show location; change color\n\nfig_100_rare_diseased_RF &lt;- fig_100_rare_diseased_RF %&gt;% \n  layout(title = '',\n         xaxis = Noax,\n         yaxis = Noax) %&gt;%\n  config(displayModeBar = FALSE)\n\nfig_100_rare_diseased_RF"
  },
  {
    "objectID": "posts/Basic_epi_quants.html#prevalence-of-exposure",
    "href": "posts/Basic_epi_quants.html#prevalence-of-exposure",
    "title": "Basic Epidemiological Quantities",
    "section": "3. Prevalence of exposure",
    "text": "3. Prevalence of exposure\nAdditionally, some risk factors (also known as “exposures”) are much more (or less). For example, consider rates of obesity, the prevalence of smoking, family history of cancer, or exposure to harmful occupational chemicals, damaging UV radiation, or poor air quality, etc. The probability that someone in a population has a specific risk factor for the disease is known as the prevalence of the risk factor.\n\n\nShow the code\n# Set up vector for diseased individuals to be colored differently\nN &lt;- 100\ndiseased_inds &lt;- union(union(seq(8,100,10),seq(9,100,10)),seq(10,100,10))\ndiseased_vec &lt;- vector(mode = \"character\", length = 100)\ndiseased_vec[diseased_inds] &lt;- 'rgb(255, 65, 54)'\ndiseased_vec[-diseased_inds] &lt;- 'rgb(66, 209, 209)'\nrisk_factor_inds &lt;- c(1:60)\nrisk_factor_vec &lt;- vector(mode = \"character\", length = 100)\nrisk_factor_vec[diseased_inds] &lt;- 'rgb(255, 65, 54)'\nrisk_factor_vec[-diseased_inds] &lt;- 'rgb(66, 209, 209)'\nrisk_factor_vec[risk_factor_inds] &lt;- 'rgb(0,0,0)' #'rgb(145, 0, 0)'\n\n\n# Set up text vector\nz_text &lt;- vector(mode = \"character\", length = 100)\nz_text[diseased_inds] &lt;- \"sick\"\nz_text[-diseased_inds] &lt;- \"healthy\"\nz_text[risk_factor_inds] &lt;- 'risk factor'\n\ndata_100_diseased_common_RF &lt;- data.frame(x = rep(seq(1,10,1), each = 10), y = rep(seq(1,10,1), times = 10), z = z_text)\n\n\nfig_100_diseased_common_RF &lt;- plot_ly(data_100_diseased_common_RF, x = ~x, y = ~y, text = ~z, type = 'scatter', mode = 'markers', \n        marker = list(size = 20, \n                      opacity = 0.7, \n                      color = diseased_vec, \n                      line = list(color = risk_factor_vec,\n                                  width = 3)),\n        hoverinfo = 'text')   # TODO: don't show location; change color\n\nfig_100_diseased_common_RF &lt;- fig_100_diseased_common_RF %&gt;% \n  layout(title = '',\n         xaxis = Noax,\n         yaxis = Noax) %&gt;%\n  config(displayModeBar = FALSE)\n\nfig_100_diseased_common_RF"
  },
  {
    "objectID": "posts/Basic_epi_quants.html#relative-risk",
    "href": "posts/Basic_epi_quants.html#relative-risk",
    "title": "Basic Epidemiological Quantities",
    "section": "4. Relative risk",
    "text": "4. Relative risk\nFinally, some risk factors are more strongly associated with a disease. For example, smoking greatly increases your risk of lung cancer (by ~ 18 times!1), while combined hormone replacement therapy only slightly increases the risk of ovarian cancer.2\n\n\nShow the code\n# Set up vector for diseased individuals to be colored differently\nN &lt;- 100\ndiseased_inds &lt;- union(union(seq(8,100,10),seq(9,100,10)),seq(10,100,10))\ndiseased_vec &lt;- vector(mode = \"character\", length = 100)\ndiseased_vec[diseased_inds] &lt;- 'rgb(255, 65, 54)'\ndiseased_vec[-diseased_inds] &lt;- 'rgb(66, 209, 209)'\nrisk_factor_inds &lt;- union(union(union(seq(8,80,10),seq(9,80,10)),seq(10,80,10)), seq(7,80,10))\nrisk_factor_vec &lt;- vector(mode = \"character\", length = 100)\nrisk_factor_vec[diseased_inds] &lt;- 'rgb(255, 65, 54)'\nrisk_factor_vec[-diseased_inds] &lt;- 'rgb(66, 209, 209)'\nrisk_factor_vec[risk_factor_inds] &lt;- 'rgb(0,0,0)' #'rgb(145, 0, 0)'\n\n\n# Set up text vector\nz_text &lt;- vector(mode = \"character\", length = 100)\nz_text[diseased_inds] &lt;- \"sick\"\nz_text[-diseased_inds] &lt;- \"healthy\"\nz_text[risk_factor_inds] &lt;- 'risk factor'\n\ndata_100_diseased_RF_RR &lt;- data.frame(x = rep(seq(1,10,1), each = 10), y = rep(seq(1,10,1), times = 10), z = z_text)\n\n\nfig_100_diseased_RF_RR &lt;- plot_ly(data_100_diseased_RF_RR, x = ~x, y = ~y, text = ~z, type = 'scatter', mode = 'markers', \n        marker = list(size = 20, \n                      opacity = 0.7, \n                      color = diseased_vec, \n                      line = list(color = risk_factor_vec,\n                                  width = 3)),\n        hoverinfo = 'text')   # TODO: don't show location; change color\n\nfig_100_diseased_RF_RR &lt;- fig_100_diseased_RF_RR %&gt;% \n  layout(title = '',\n         xaxis = Noax,\n         yaxis = Noax) %&gt;%\n  config(displayModeBar = FALSE)\n\nfig_100_diseased_RF_RR\n\n\n\n\n\n\nThe strength of the association between the risk factor and the disease is known as the relative risk (RR):\n\\[\n\\begin{equation}\nRR = \\frac{\\Pr(D | E)}{\\Pr(D | \\bar{E})}\n\\end{equation}\n\\tag{1}\\]\nwhere \\(\\Pr(D | E)\\) is the probability that someone is diseased given that they were exposed to the risk factor, and \\(\\Pr(D | \\bar{E})\\) is the probability that someone is diseased given that they were not exposed to the risk factor. We can estimate these probabilities using observed quantities:\n\\[\n\\begin{equation}\n\\widehat{RR} = \\frac{\\frac{\\text{\\# exposed people with disease}}{\\text{total \\# exposed people}}}{\\frac{\\text{\\# unexposed people with disease}}{\\text{total \\# unexposed people}}}\n\\end{equation}\n\\]"
  },
  {
    "objectID": "posts/PRS_CLT_demo.html",
    "href": "posts/PRS_CLT_demo.html",
    "title": "Central Limit Theorem Demo with Polygenic Risk Scores",
    "section": "",
    "text": "The classical Central Limit Theorem (CLT) states that for a series of independent and identically distributed random variables \\(X_1, X_2, ...\\) with \\(\\mathbb{E}[X_i] = \\mu\\) and \\(var(X_i) = \\sigma^2 &lt; \\infty\\), a finite sum of those variables \\(S_n = X_1 + X_2 + ... + X_n\\) will be normally distributed and centered at \\(n \\mu\\). Formally, we can write the CLT as follows: \\[\n\\frac{S_n - n \\mu}{\\sigma \\sqrt{n}} \\to^D \\mathcal{N}(0,1)\n\\] where \\(\\to^D\\) indicates convergence in distribution.\nThere exist several other versions of the CLT that relax the assumptions of independence and/or identical distribution of the random variables. In this demonstration, we consider random variables that are independent but not identically distributed, effectively demonstrating Lyapunov’s CLT, which does not require the random variables to be identically distributed."
  },
  {
    "objectID": "posts/PRS_CLT_demo.html#central-limit-theorem",
    "href": "posts/PRS_CLT_demo.html#central-limit-theorem",
    "title": "Central Limit Theorem Demo with Polygenic Risk Scores",
    "section": "",
    "text": "The classical Central Limit Theorem (CLT) states that for a series of independent and identically distributed random variables \\(X_1, X_2, ...\\) with \\(\\mathbb{E}[X_i] = \\mu\\) and \\(var(X_i) = \\sigma^2 &lt; \\infty\\), a finite sum of those variables \\(S_n = X_1 + X_2 + ... + X_n\\) will be normally distributed and centered at \\(n \\mu\\). Formally, we can write the CLT as follows: \\[\n\\frac{S_n - n \\mu}{\\sigma \\sqrt{n}} \\to^D \\mathcal{N}(0,1)\n\\] where \\(\\to^D\\) indicates convergence in distribution.\nThere exist several other versions of the CLT that relax the assumptions of independence and/or identical distribution of the random variables. In this demonstration, we consider random variables that are independent but not identically distributed, effectively demonstrating Lyapunov’s CLT, which does not require the random variables to be identically distributed."
  },
  {
    "objectID": "posts/PRS_CLT_demo.html#lyapunovs-central-limit-theorem",
    "href": "posts/PRS_CLT_demo.html#lyapunovs-central-limit-theorem",
    "title": "Central Limit Theorem Demo with Polygenic Risk Scores",
    "section": "Lyapunov’s Central Limit Theorem",
    "text": "Lyapunov’s Central Limit Theorem\nLyapunov’s CLT relaxes the assumption that the random variables are identically distributed. It states that for a sequence of independent random variables \\(X_1, X_2, ...\\), if the \\((2+\\epsilon)^{th}\\) moment (for \\(\\epsilon &gt; 0\\)) exists, and \\(\\mathbb{E}[X_i] = \\mu_i &lt; \\infty\\) and \\(var(X_i) = \\sigma_i^2 &lt; \\infty\\), we define\n\\[\nr_n^{2+\\epsilon} = \\sum_{i=1}^{n} \\langle \\lvert x_i - \\mu_i \\rvert ^{2+\\epsilon} \\rangle\n\\] \\[\ns_n^2 = \\sum_{i=1}^n \\sigma_i^2\n\\] then if\n\\[\n\\lim_{n \\to \\infty} \\frac{r_n}{s_n} = 0\n\\] then the CLT holds."
  },
  {
    "objectID": "posts/PRS_CLT_demo.html#polygenic-risk-scores",
    "href": "posts/PRS_CLT_demo.html#polygenic-risk-scores",
    "title": "Central Limit Theorem Demo with Polygenic Risk Scores",
    "section": "Polygenic Risk Scores",
    "text": "Polygenic Risk Scores\nTo predict someone’s risk of cancer, statisticians can use a polygenic risk score (PRS) to identify individuals with a high genetic risk of cancer who may benefit from early or extra monitoring. A PRS is a weighted sum of harmful (or protective) genetic variants that may increase (or decrease) someone’s risk of disease. Formally, we define a PRS as follows:\n\\[\nPRS = \\sum_{i=1}^{n} \\beta_i G_i\n\\] where \\(\\beta_i\\) is the effect weight (i.e. effect size) and \\(G_i \\in\\{0,1,2\\}\\) is the genotype for \\(i = 1,...,n\\) genetic variants.\nBy construction, \\(G_i\\) are independent (i.e. we only include independent alleles, which we know by calculating their linkage disequilibrium scores), and they have finite mean and variance:\n\\[\\begin{align*}\n\\mathbb{E}[PRS] &= \\mathbb{E}[\\sum_{i=1}^{n}\\beta_i G_i] \\\\\n&= \\sum_{i=1}^{n}\\mathbb{E}[\\beta_i G_i] \\because \\text{independent} \\\\\n&= \\sum_{i=1}^{n}\\beta_i \\mathbb{E}[G_i] \\\\\n&= \\sum_{i=1}^{n} \\beta_i 2p_i &lt; \\infty\n\\end{align*}\\]\n\\[\\begin{align*}\nvar(PRS) &= var(\\sum_{i=1}^{n} \\beta_i G_i) \\\\\n&= \\sum_{i=1}^{n} var(\\beta_i G_i) \\because \\text{independent} \\\\\n&= \\sum_{i=1}^{n} \\beta_i^2 var(G_i) \\\\\n&= \\sum_{i=1}^{n} \\beta_i^2 2 p_i (1-p_i) &lt; \\infty\n\\end{align*}\\]\nHowever, the genotypes are not identically distributed: \\[\nG_i \\sim Bin(2,p_i)\n\\] where \\(p_i \\in[0,1]\\) is the frequency of the \\(i^{th}\\) effect allele.\nNotice that the PRS meets the assumptions of independence and finite mean and variance, but the random variables are not identically distributed. Since the \\((2+\\epsilon)^{th}\\) moment is difficult to obtain, we will explore the behavior of the PRS through simulation. We consider a series of PRS for various common cancers that have a different number of risk alleles included in the PRS (\\(n\\)). Typically, we calculate PRS on real people’s genotype data, but for the sake of privacy, we will simulate data based on the allele frequencies provided for each PRS."
  },
  {
    "objectID": "posts/PRS_CLT_demo.html#simulated-prs-simple-to-complex",
    "href": "posts/PRS_CLT_demo.html#simulated-prs-simple-to-complex",
    "title": "Central Limit Theorem Demo with Polygenic Risk Scores",
    "section": "Simulated PRS: Simple to complex",
    "text": "Simulated PRS: Simple to complex\nPRS are publicly available via the PGS Catalog. The typical PGS ‘scorefile’ (shown below for thyroid cancer) includes enough information to identify a specific variant (e.g. rsID or chromosome number and position), an indication of which allele is the effect allele and which is not (i.e. the ‘other’ allele) for purposes of cross-checking in the event of genotyping complexity (e.g. strand-flipping or multi-allelic variants), and the effect size estimate (\\(\\hat{\\beta_i}\\)). Some scorefiles also contain information about how common the effect allele is, which is useful for inferring ambiguous SNPs, and required for simulating genotypes.\nAs we showed above, \\(G_i \\sim Bin(2,p_i)\\) where \\(p_i \\in[0,1]\\) is the frequency of the effect allele, so we can use the empirical effect allele frequency to simulate \\(G_i\\) for a large number of individuals (N = 10,000). Then we will calculate the PRS using the formula above and check out the distribution to see if it is roughly normal. Let’s begin with a relatively simple PRS.\n\nThyroid cancer\nThyroid cancer ranks as ~13th most common cancer in the U.S., with ~44,000 new cases predicted for 2024.1 Graff et al. (2021)2 developed a relatively simple PRS for thyroid cancer (PGS000087) that only includes 12 variants:\n\n\nShow the code\n# Define function to make nice table\nmake_table &lt;- function(PGS_ID) {\n  # Read in scorefile for the PRS\n  scorefile &lt;- read_csv(here(\"data\",paste0(PGS_ID,\"_scorefile.csv\")))\n  \n  # Reformat to make pretty table \n  scorefile_pretty &lt;- scorefile %&gt;%\n    mutate(effect_weight = round(effect_weight, 4)) %&gt;%\n    rename(Chr = chr_name,\n           Position = chr_position,\n           'Effect allele' = effect_allele,\n           'Other allele' = other_allele,\n           'Effect weight' = effect_weight,\n           'Effect allele freq.' = allelefrequency_effect)\n  \n  return(datatable(scorefile_pretty, options = list(pageLength = 25)))\n} \n\n# Make table for thyroid PRS\nPRS_id_thyroid &lt;- \"PGS000087\"\nmake_table(PRS_id_thyroid)\n\n\n\n\n\n\n\n\nShow the code\n# Define function to simulate PRS\nsim_PRS &lt;- function(PGS_ID, N_sim, subset = NA) {\n  # Load scorefile for PRS\nscorefile &lt;- read_csv(here(\"data\",paste0(PGS_ID,\"_scorefile.csv\")))\n\n# If we want to subset the PRS variants randomly, do it here\nif (is.na(subset) == FALSE) {\n inds &lt;- sample(1:dim(scorefile)[1],subset)\n  scorefile &lt;- scorefile[inds,]\n}\n\n# Simulate genotypes for each SNP\ndata &lt;- mapply(rbinom, prob = scorefile$allelefrequency_effect, MoreArgs = list(n = N_sim, size = 2))\n\n# Check that data were simulated correctly:\n# colMeans(data)/2 - scorefile$allelefrequency_effect\n\n# Calculate PRS\nPRS &lt;- colSums(t(data)*scorefile$effect_weight)\n\nreturn(PRS)\n}  # end sim_PRS function\n\n# calculate expected mean and variance of PRS (assuming beta_hat = beta)\ncalc_exp_mean_var &lt;- function(PGS_ID) {\n  # Load scorefile for PRS\nscorefile &lt;- read_csv(here(\"data\",paste0(PGS_ID,\"_scorefile.csv\")))\n\np &lt;- scorefile$allelefrequency_effect\nbeta &lt;- scorefile$effect_weight\n\nmu &lt;- sum(2*beta*p)\nsigmasq &lt;- sum(2*(beta)^2*p*(1-p))\nreturn(list(mu = mu, sigmasq = sigmasq))\n}\n\n# function to make pretty PRS plot\nplot_PRS &lt;- function(PRS_scores, mu = NA) {\n  PRS_df &lt;- data.frame(PRS = PRS_scores)\n  plot &lt;- PRS_df %&gt;%\n    ggplot(aes(x = PRS)) +\n   # geom_histogram() +\n    geom_density() +\n    theme_minimal()\n  \n  return(plot)\n}\n\n# Simulate PRS for thyroid\n# Number of individuals to simulate genotypes for\nN_sim &lt;- 10000\n\n# nvar = 12\nPRS_thyroid &lt;- sim_PRS(PRS_id_thyroid, N_sim)\n\n# Calc expected mean and var\nPRS_thyroid_exp = calc_exp_mean_var(PRS_id_thyroid)\n\n# Plot distribution of betas\nscorefile_thyroid &lt;- read_csv(here(\"data\",paste0(PRS_id_thyroid,\"_scorefile.csv\")))\n\nhist(scorefile_thyroid$effect_weight, xlab = \"Effect weight\", main = \"Histogram of thyroid cancer PRS effect weights\")\n\n\n\n\n\n\n\n\n\nShow the code\n# Calculate summary statistics\nPRS_thyroid_mn &lt;- mean(PRS_thyroid)\nPRS_thyroid_var &lt;- var(PRS_thyroid)\n\n\n# Plot PRS \nh &lt;- hist(PRS_thyroid, breaks=30, xlab = \"PRS\", main = \"Simulated PRS for thyroid cancer\")\nxfit &lt;- seq(min(PRS_thyroid), max(PRS_thyroid),length=40)\nyfit &lt;- N_sim/5*dnorm(xfit, mean = mean(PRS_thyroid), sd = sd(PRS_thyroid)) \nlines(xfit,yfit)\n\n\n\n\n\n\n\n\n\n\n\nLung cancer\nLung cancer is the number one cause of cancer deaths. This lung cancer PRS (PGS000740), developed by Hung et al. (2021),3 has 128 genetic variants. This histogram looks even more symmetric and is becoming more normal.\n\n\nShow the code\n# nvar = 128\nPRS_id_lung &lt;- \"PGS000740\"\nPRS_lung &lt;- sim_PRS(PRS_id_lung, N_sim)\n\n# Plot distribution of betas\nscorefile_lung &lt;- read_csv(here(\"data\",paste0(PRS_id_lung,\"_scorefile.csv\")))\nhist(scorefile_lung$effect_weight, xlab = \"Effect weight\", main = \"Histogram of lung cancer PRS effect weights\")\n\n\n\n\n\n\n\n\n\nShow the code\n# Calc expected mean and var\nPRS_lung_exp = calc_exp_mean_var(PRS_id_lung)\n\n# Calculate summary statistics\nPRS_lung_mn &lt;- mean(PRS_lung)\nPRS_lung_var &lt;- var(PRS_lung)\n\n# Plot PRS \nh &lt;- hist(PRS_lung, breaks=30, xlab = \"PRS\", main = \"Simulated PRS for lung cancer\")\nxfit &lt;- seq(min(PRS_lung), max(PRS_lung),length=40)\nyfit &lt;- N_sim/10*dnorm(xfit, mean = mean(PRS_lung), sd = sd(PRS_lung)) \nlines(xfit,yfit)\n\n\n\n\n\n\n\n\n\n\n\nBreast cancer\nFinally, let’s look at the breast cancer PRS developed by Mavaddat et al. (2019).4 This PRS (PGS000004) contains 313 variants. The distribution of this PRS looks very normal!\n\n\nShow the code\n# nvar = 313\nPRS_id_breast &lt;- \"PGS000004\"\nPRS_breast &lt;- sim_PRS(PRS_id_breast, N_sim)\n\n# Plot distribution of betas\nscorefile_breast &lt;- read_csv(here(\"data\",paste0(PRS_id_breast,\"_scorefile.csv\")))\nhist(scorefile_breast$effect_weight, xlab = \"Effect weight\", main = \"Histogram of breast cancer PRS effect weights\")\n\n\n\n\n\n\n\n\n\nShow the code\n# Calc expected mean and var\nPRS_breast_exp = calc_exp_mean_var(PRS_id_breast)\n\n# Calculate summary statistics\nPRS_breast_mn &lt;- mean(PRS_breast)\nPRS_breast_var &lt;- var(PRS_breast)\n\n# Plot PRS \nh &lt;- hist(PRS_breast, breaks=30, xlab = \"PRS\", main = \"Simulated PRS for breast cancer\")\nxfit &lt;- seq(min(PRS_breast), max(PRS_breast),length=40)\nyfit &lt;- N_sim/5*dnorm(xfit, mean = mean(PRS_breast), sd = sd(PRS_breast)) \nlines(xfit,yfit)\n\n\n\n\n\n\n\n\n\nIf you’re like me and skeptical that all of these PRS were too beautiful (i.e. normal) especially when the number of genetic variants was small (!!), then I’ll demonstrate the CLT by subsampling a random set of variants from the 313 breast cancer PRS.\n\n\nShow the code\n# Repeat with different number of variants\nsubsets &lt;- c(5, 15, 30, 75, 150, 313)\nsubset_labels &lt;- as_labeller(c('5' = \"n = 5\", '15' = \"n = 15\", '30' = \"n = 30\", '75' = \"n = 75\", '150' = \"n = 100\", '313' = \"n = 313\"))\n\nPRS_breast_subset &lt;- mapply(sim_PRS, subset = subsets, MoreArgs = list(PGS_ID = PRS_id_breast, N_sim = N_sim))\nPRS_breast_subset_df &lt;- data.frame(PRS = as.vector(PRS_breast_subset))\nPRS_breast_subset_df$n_var &lt;- rep(subsets,each = N_sim)\n\n# Plot\nPRS_breast_subset_df %&gt;%\n  ggplot() +\n  geom_histogram(aes(x = PRS)) +\n  facet_wrap(vars(n_var), labeller = subset_labels) + \n  theme_minimal() +\n  labs(title = \"PRS for a subset of variants from the 313 breast cancer model\",\n       subtitle = \"The distributions become normal and centered at the expected mean as n increases\")"
  },
  {
    "objectID": "posts/PRS_CLT_demo.html#summary-and-importance",
    "href": "posts/PRS_CLT_demo.html#summary-and-importance",
    "title": "Central Limit Theorem Demo with Polygenic Risk Scores",
    "section": "Summary and importance",
    "text": "Summary and importance\nNotice that the shape of the PRS curves became more normal as the number of variants increased. Additionally, is ~ normal with mean and sd similar to what we would expect under the CLT! This is because of the additive nature of the PRS. In research, we rely on this predictable behavior of the PRS to simulate PRS for reference populations when those individuals do not have genotpye data available."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Emily L. Norton",
    "section": "",
    "text": "I am a 4th year PhD candidate in the Department of Biostatistics in the Bloomberg School of Public Health at Johns Hopkins University. My research focuses on understanding how genetics and environmental factors influence disease risk, especially cancer, and how this risk can be predicted equitably across populations."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Example analysis",
    "section": "",
    "text": "The dark-eyed junco was the most commonly observed species in Project FeederWatch in winter 2020-21.  Photo credit: Bob Vuxinic, submitted to Project FeederWatch\n\n\n\nProject aims\nIn this example analysis, I investigate patterns in abundance and distribution of birds in the 2020-21 winter in the United States and southern Canada. Specifically, I aim to answer the following questions:\n\nwhen and where are birds observed\nwhich species are most common, and where are they found\nwhich species tend to occur in large flocks, and where are they found\n\nThis analysis is intended for the general public, as well as the citizen scientists (and the graders for Biostat 777).\n\n\nData from Project FeederWatch\nThese data were discovered via TidyTuesday for January 10, 2023, but originally come from Project FeederWatch. Project FeederWatch is a citizen science project that aims to engage individuals in North America to count birds (for as long or as little as they like) to track winter trends in bird distribution and abundance. This project has been running for more than 30 years at this point!1 The data dictionary is available here.\n\n\nShow the code\n# Load data and do quick exploration/clean-up\n\n# List required files to download from TidyTuesday\nrds_files &lt;- c(\"feederwatch.RDS\",\"site_data.RDS\")\n\n# Check if any of these files don't exist\nif (any(!file.exists(here(\"data\", rds_files)))) {\n  dir.create(here(\"data\"))\n# if missing, download the data\nfeederwatch &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-01-10/PFW_2021_public.csv')\nsite_data &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-01-10/PFW_count_site_data_public_2021.csv')\n\n# save the data objects as RDS files\nsaveRDS(feederwatch, file = here(\"data\",\"feederwatch.RDS\"))\nsaveRDS(site_data, file = here(\"data\",\"site_data.RDS\"))\n}\n\n# Load data\nfeederwatch &lt;- readRDS(here(\"data\",\"feederwatch.RDS\"))\nfeederwatch_og &lt;- feederwatch\nsite_data &lt;- readRDS(here(\"data\",\"site_data.RDS\"))\nsite_data_og &lt;- site_data\n# had to save the species_codes dictionary manually from https://docs.google.com/spreadsheets/d/1kHmx2XhA2MJtEyTNMpwqTQEnoa9M7Il2/edit#gid=2040245914\nspecies_codes_dd &lt;- read_csv(here(\"data\",\"FeederWatch_Data_Dictionary_Species_Codes.csv\"),skip = 1)\n\n\n# Quick data exploration, clean up, and creation of a few useful data frames \n# glimpse(feederwatch)\n# summary(feederwatch)\n#length(unique(feederwatch$species_code))\n\n# Change format for names in data dictionary:\nspecies_codes_dd$PRIMARY_COM_NAME &lt;- str_to_title(species_codes_dd$PRIMARY_COM_NAME)\n\n# Check for outliers in space\nlow_lat &lt;- which(feederwatch$latitude&lt;0)   # length=1; this point appears to be erroneous - remove it below\nhigh_lat &lt;- which(feederwatch$latitude&gt;50) # these are not erroneous, but want to narrow to US for plotting later on\n\n# Get rid of the one low-latitude observation\nfeederwatch &lt;- feederwatch[-low_lat,]\n# hist(feederwatch$latitude)\n\n# Add latitudinal bands to the data\nfeederwatch_df &lt;- feederwatch %&gt;%\n  mutate(Date = ymd(paste(Year, Month, Day, sep=\"-\")),\n         lat_band = case_when(\n           latitude &lt; 35 ~ \"low\",\n           latitude &gt;=35 & latitude &lt; 45 ~ \"mid\",\n           latitude &gt;= 45 ~ \"high\"\n         )) \nfeederwatch_df$lat_band &lt;- factor(feederwatch_df$lat_band, levels=c(\"high\",\"mid\",\"low\"))\n\n# Make useful dataframes:\n# Total bird count per day \nbird_count_per_day &lt;- feederwatch_df %&gt;%\n  group_by(Date, \n           lat_band) %&gt;%\n  summarize(total = sum(how_many),\n            count = n(),\n            std_total = total/count)\n\n# Abundant birds: birds that have highest counts in the dataset - i.e. flocking birds\nabundant_birds &lt;- feederwatch_df %&gt;%\n  filter(latitude&lt;50) %&gt;%\n  group_by(species_code) %&gt;%\n  summarize(total = sum(how_many),\n            count = n(),\n            std_total = total/count) %&gt;%\n  arrange(desc(std_total)) %&gt;%\n  head(n=9) %&gt;%\n  left_join(species_codes_dd, \n            by=join_by(species_code==SPECIES_CODE)) %&gt;%\n  select(species_code, \n         PRIMARY_COM_NAME, \n         SCI_NAME, count)\n\n# join with species info:\nabundant_birds_df &lt;- feederwatch_df %&gt;%\n   filter(species_code %in% abundant_birds$species_code) %&gt;%\n   left_join(species_codes_dd, \n             by=join_by(species_code==SPECIES_CODE)) %&gt;%\n   mutate(Month=factor(Month, levels=c(\"11\",\"12\",\"1\",\"2\",\"3\",\"4\")))\n\n\n# Common birds: birds that have several records in the dataset\ncommonly_sighted_birds &lt;- feederwatch_df %&gt;%\n  group_by(species_code) %&gt;%\n  summarize(total = sum(how_many),\n            count = n(),\n            std_total = total/count) %&gt;%\n  arrange(desc(count)) %&gt;%\n  head(n=9) %&gt;%\n  left_join(species_codes_dd, \n            by=join_by(species_code==SPECIES_CODE)) %&gt;%\n  select(species_code, \n         PRIMARY_COM_NAME, \n         SCI_NAME, \n         count)\n\n# join with species info:\ncommonly_sighted_birds_df &lt;- feederwatch_df %&gt;%\n   filter(species_code %in% commonly_sighted_birds$species_code) %&gt;%\n   left_join(species_codes_dd, \n             by=join_by(species_code==SPECIES_CODE)) %&gt;%\n   mutate(Month=factor(Month, levels=c(\"11\",\"12\",\"1\",\"2\",\"3\",\"4\")))\n\n\n\n\nWhere are birds observed?\nIn order to understand trends in bird distribution and abundance, we first need to know where our observations are coming from. Here is a figure showing the locations of observations. The points are transparent, so denser bird-watching areas are shaded darker green.\n\n\nShow the code\n# Figure 1: Map of unique sighting locations\n\n# Prep maps of America/USA for making figures\namerica_map &lt;- map_data(\"world\", region='USA')\nUSA_map &lt;-\n  ggplot(america_map, \n         aes(x=long, y=lat, group=group)) +\n  geom_polygon(fill=\"white\",color=\"black\") + \n  scale_x_continuous(limits = c(-125,-65)) +\n  scale_y_continuous(limits = c(25, 50)) + \n  coord_map() +\n  theme_minimal()\n\n# Identify unique sighting locations\nunique_sighting_locations &lt;- feederwatch %&gt;%\n  group_by(loc_id) %&gt;%\n  summarize(n_obs = n(),\n            latitude=mean(latitude),\n            longitude=mean(longitude))\n\n# Plot unique sighting locations\nUSA_map +\n  geom_point(data=unique_sighting_locations, \n             aes(x = longitude, y = latitude, group=NULL), \n             alpha=0.1, \n             color=\"darkgreen\") +\n  labs(x=\"Longitude (ºE)\", \n       y = \"Latitude (ºN)\", \n       title = \"Observing locations for 2020-21 winter in US and southern Canada\",\n       subtitle = \"Engagement in Project Feederwatch is stronger in the eastern than the western U.S., \\nand the highest density of birdwatchers are in the mid-Atlantic to Northeast regions.\",\n       caption = \"Data: Project FeederWatch\")\n\n\n\n\n\n\n\n\nFigure 1: Observing locations for Project FeederWatch 2020-21 winter.\n\n\n\n\n\n\n\n\n\n\n\nBirdwatching locations\n\n\n\nParticipants can birdwatch anywhere – it doesn’t have to be at their backyard feeder! But remember the distribution of observing locations in this figure since it will be relevant for understanding the distribution of common and flocking birds examined below.\n\n\nSince most bird-watching happens in the mid-latitudes (defined here as 35-45ºN), we must standardize the number of birds by the number of observations. The following figure indicates that birds are more abundant at higher latitudes even though there are fewer total observations. However, birds become more common at lower latitudes (&lt;35ºN) from January to March. This may be explained by migration to southern regions during the coldest months of the year. The fact that more birds are reported within every latitudinal band during the coldest months may indicate that more birds visit feeders to obtain food during the harshest conditions of the year.\n\n\n\n\n\nShow the code\n# Figure 2: Bird count per latitude band\nbird_count_per_day %&gt;%\n  ggplot(aes(x=Date, y = std_total, color = lat_band)) +   \n  geom_smooth() +   # loess-smoother is default\n  theme_minimal() +   \n  labs(y = \"Average number of birds per observation\", \n       title = \"Birds observed per latitude band during 2020-21 winter\", \n       color = \"Latitude band\", \n       subtitle = \"Birds are most abundant at high latitudes, but become relatively \\nmore frequent in low latitudes in the winter\", \n       caption = \"Data: Project FeederWatch\")\n\n\n\n\n\n\n\n\nFigure 2: Total number of birds observed within each latitudinal band (high: &gt;=45ºN, mid: 35-45ºN, low: &lt;35ºN).\n\n\n\n\n\n\n\nLOESS (locally weighted running line smoother) was used on these curves. This is the default option for geom_smooth.\n\n\n\nLet’s now explore how often different species are observed. A total of 361 unique species were reported, but most of these species were only observed once! However, the most common species were observed thousands of times! This is probably explained in part because birds are incredibly diverse,2 so some of these species may actually be rare, while others have may have limited distributions or be harder to identify.\n\n\nShow the code\n# Figure 3: Histogram of bird observations per species\nfeederwatch_df %&gt;%\n  group_by(species_code) %&gt;%\n  summarize(n_obs = n()) %&gt;%\n  ggplot() +\n  geom_histogram(aes(n_obs), \n                 boundary=0, \n                 closed=\"left\") +\n  labs(x = \"Number of observations for a given species\", \n       y = \"Frequency\", \n       title = \"Number of observations per bird species\",\n       subtitle = \"Most bird species are only observed once. A few common species are observed \\nthousands of times.\",\n       caption = \"Data: ProjectFeederwatch\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 3: Histogram of the number of observations per species.\n\n\n\n\n\n\n\n\n\n\n\nRare birds are becoming rarer\n\n\n\nIn 2019, the National Audubon Society estimated that two-thirds of North American birds were at risk of extinction due to climate change.3\n\n\n\n\nWhich birds are most common?\nYou’ll probably recognize the names of some of the most common birds! These birds were identified as “common” in the data set because they had the most observations (though not necessarily the highest total count of individuals).\n\n\nShow the code\n# Table 1\ncommonly_sighted_birds_table &lt;- commonly_sighted_birds %&gt;%\n  select('Common name' = PRIMARY_COM_NAME,\n         'Scientific name' = SCI_NAME,\n         'Number of observations' = count)\ncommonly_sighted_birds_table\n\n\n\nMost commonly observed birds in the 2020-21 winter in Project FeederWatch.\n\n\n\n\n\n\n\nCommon name\nScientific name\nNumber of observations\n\n\n\n\nDark-Eyed Junco\nJunco hyemalis\n5465\n\n\nDowny Woodpecker\nPicoides pubescens\n5449\n\n\nNorthern Cardinal\nCardinalis cardinalis\n5438\n\n\nHouse Finch\nHaemorhous mexicanus\n4955\n\n\nMourning Dove\nZenaida macroura\n4938\n\n\nBlack-Capped Chickadee\nPoecile atricapillus\n4696\n\n\nBlue Jay\nCyanocitta cristata\n4578\n\n\nAmerican Goldfinch\nSpinus tristis\n4446\n\n\nWhite-Breasted Nuthatch\nSitta carolinensis\n4393\n\n\n\n\n\nMost of the common birds were observed throughout the U.S. (or at least where people were reportedly birdwatching, see Figure 1). However, some birds had more limited distributions. For example, the blue jay and the northern cardinal were found predominantly in the eastern half of the U.S., and the black-capped chickadee was only observed in the more northern latitudes.\n\n\nShow the code\n# Figure 4: Commonly sighted birds\n# Since this figure takes a long time to render, we'll just load in the saved figure below, but this is the code that was used to generate it:\n  \ncommon_birds_plot &lt;- USA_map +\n  geom_bin_2d(data = commonly_sighted_birds_df, \n              aes(x = longitude, y = latitude, group=NULL)) +\n  scale_fill_continuous(low=\"lightskyblue1\", \n                        high=\"indianred4\", \n                        name=\"Number of \\nsightings\") +\n  theme_minimal() + \n  labs(x=\"Longitude (E)\", \n       y=\"Latitude (N)\", \n       title=\"Observations of common birds in the US and southern Canada\", \n       subtitle=\"Common bird species are most frequently observed in the Mid-Atlantic and \\nNortheast regions, likely due (in part) to a sampling bias and easy bird identification\", \n       color=\"Month\") + \n  facet_wrap(vars(PRIMARY_COM_NAME), \n             labeller = labeller(PRIMARY_COM_NAME = label_wrap_gen(width = 10)))\n#ggsave(here(\"figs\",\"Commonly_sighted_birds.png\"), plot=common_birds_plot, width=7, height=7, units=\"in\")\n\n\n\n\n\n\n\n\nFigure 4: Heatmap of observations for the most common birds.\n\n\n\n\n\nWhich birds are usually found in a flock?\nHave you heard the saying that “birds of a feather flock together”? The birds in the figure below had the highest average abundance per observation. This indicates to me that these birds tend to appear most often in large flocks.\n\n\nShow the code\n# Figure 5: Most abundant birds (i.e. probably largest flocks)\n# focus on birds in US (or below 50oN):\nmonth_colors = c(\"sandybrown\",\"indianred\",\"slateblue3\",\"royalblue\",\"palegreen3\")\n\nUSA_map +\n  geom_point(data = abundant_birds_df, \n             aes(x = longitude, y = latitude, group=NULL, size=how_many, color=Month), \n             alpha=0.5) +\n  scale_color_manual(values=month_colors) +   \n  theme_minimal() + \n  labs(x=\"Longitude (E)\", \n       y=\"Latitude (N)\", \n       title=\"Flocking birds observed in the US and southern Canada\", \n       subtitle=\"Birds with the highest average count per observation \\ntend to have limited distributions\", \n       caption = \"Data: Project FeederWatch\",\n       size=\"Count per observation\") +\n  facet_wrap(vars(PRIMARY_COM_NAME), \n             labeller = labeller(PRIMARY_COM_NAME = label_wrap_gen(width = 10)))\n\n\n\n\n\n\n\n\nFigure 5: Month and location for bird species with the largest average count per observation. The size of the circle is relative to the flock size, and the color indicates the month when the observation was made.\n\n\n\n\n\n\n\nSummary\nIn this example analysis, we explored patterns in the distribution and abundance of birds observed through the citizen science initiative Project FeederWatch conducted in winter 2020-21. We found that most of the observations for this program were made in the mid-Atlantic and northeast regions of the U.S. This pattern of observation corresponded strongly with the distribution of several of the most common birds, including the American goldfinch, dark-eyed junco, downy woodpecker, and house finch. However, flocking birds tended to have smaller ranges but a larger number of birds reported per observation. In general, birds were most abundant in the coldest months (January-March) and at the highest latitudes (&gt;45ºN).\n\n\nFunctions used\ndplyr: mutate, group_by, summarize, arrange, filter\nggplot2: geom_smooth, geom_histogram, geom_polygon, geom_bin_2d, geom_point, facet_wrap\n\n\n\n\n\nReferences\n\n1. Bonter, D. N. & Grieg, E. I. Over 30 years of standardized bird counts at supplementary feeding stations in north america: A citizen science data report for project FeederWatch. Frontiers in Ecology and Evolution 31, (2021).\n\n\n2. Wong, K. How birds evolved their incredible diversity. Scientific American (2020).\n\n\n3. Wilsey, C. et al. Survival by degrees: 389 bird species on the brink. (2019)."
  }
]